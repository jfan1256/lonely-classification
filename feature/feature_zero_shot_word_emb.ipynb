{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19664,
     "status": "ok",
     "timestamp": 1689815845648,
     "user": {
      "displayName": "Raghav Aggarwal",
      "userId": "04371493702700305700"
     },
     "user_tz": 240
    },
    "id": "2bYHIJwSGEIn",
    "outputId": "b78b21e4-e020-4d44-eb1f-bd2c1530ab12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\weigfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ssl\n",
    "import nltk\n",
    "import re\n",
    "import time \n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib \n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from utils.system import *\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# To suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"the specific warning message\", module=\"the_module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    # Correcting common abbreviations and numbers with periods to avoid incorrect splits\n",
    "    text = re.sub(r'\\b(e.g.|i.e.|etc.)\\b', lambda x: x.group().replace('.', ''), text)\n",
    "    text = re.sub(r'(\\d+)\\.', r'\\1', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize the corrected text into sentences\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Function to get word embedding using mean pooling\n",
    "def get_word_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        outputs = model(**inputs)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    word_embedding = mean_pooling(outputs, attention_mask)\n",
    "    return word_embedding.squeeze().numpy()\n",
    "\n",
    "# Function to find top N closest words\n",
    "def cosine_similarity_for_words(word, word_list):\n",
    "    word_embedding = get_word_embedding(word)\n",
    "    closest_words = []\n",
    "\n",
    "    for other_word in word_list:\n",
    "        other_word_embedding = get_word_embedding(other_word)\n",
    "        similarity = 1 - cosine(word_embedding, other_word_embedding)\n",
    "        closest_words.append((other_word, similarity))\n",
    "\n",
    "    return closest_words\n",
    "\n",
    "\n",
    "# Function to find top N closest words\n",
    "def top_n_closest_words(word, word_list, n=5):\n",
    "    word_embedding = get_word_embedding(word)\n",
    "    closest_words = []\n",
    "\n",
    "    for other_word in word_list:\n",
    "        other_word_embedding = get_word_embedding(other_word)\n",
    "        similarity = 1 - cosine(word_embedding, other_word_embedding)\n",
    "        closest_words.append((other_word, similarity))\n",
    "\n",
    "    # Sort by similarity\n",
    "    closest_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return closest_words[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  overall_label  \\\n",
      "id                                                                       \n",
      "0     Red streak girlSo uh yeah she seems pretty chi...              0   \n",
      "1            Just wanna talkIm here to talk if you want              0   \n",
      "2     How to get over jealousy of socially active fr...              0   \n",
      "3     Were all lonely people, right?Saw a post on he...              0   \n",
      "4     i hate my birthdaymy birthday is in two days a...              1   \n",
      "...                                                 ...            ...   \n",
      "7994  HelloHow are you today? \\r\\r\\nAnd how was your...              0   \n",
      "7995  I have nobody and nothing to live for.At this ...              1   \n",
      "7996  26M [Friendship] - Clean audio chat - Depressi...              0   \n",
      "7997  I forgot how to make friendsIt's been so long ...              1   \n",
      "7998  Please.Hi. If anyone can keep me company tonig...              0   \n",
      "\n",
      "                                        cleaned_article  \n",
      "id                                                       \n",
      "0     red streak girl. so uh yeah she seems pretty c...  \n",
      "1         just wanna talk. i'm here to talk if you want  \n",
      "2     how to get over jealousy of socially active fr...  \n",
      "3     were all lonely people, right? saw a post on h...  \n",
      "4     i hate my birthdaymy birthday is in two days a...  \n",
      "...                                                 ...  \n",
      "7994   hello. how are you today? and how was your day?   \n",
      "7995  i have nobody and nothing to live for. at this...  \n",
      "7996  26m [friendship] - clean audio chat - depressi...  \n",
      "7997  i forgot how to make friends. it's been so lon...  \n",
      "7998  please. hi. if anyone can keep me company toni...  \n",
      "\n",
      "[7999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(get_data() / 'clean_data.parquet.brotli')\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Lonely Articles\n",
    "lonely_data = data.copy(deep=True)\n",
    "lonely_data = lonely_data.reset_index(drop=True)\n",
    "lonely_data['id'] = lonely_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "lonely_data['sentences'] = lonely_data['cleaned_article'].apply(split_into_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined all sentences into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i, text in enumerate(lonely_data['sentences']):\n",
    "    post = text\n",
    "    post = ' '.join(post)\n",
    "    corpus.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Consine Similarity when ngram = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 3000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_feature=3000\n",
    "cv=CountVectorizer(max_features = size_of_feature, stop_words='english', ngram_range=(1, 1))\n",
    "X=cv.fit_transform(corpus)\n",
    "feat_dict=cv.get_feature_names_out()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Cosine Similarity of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 236.8773136138916\n"
     ]
    }
   ],
   "source": [
    "# Apply the get_bert_embedding function to your cleaned sentences\n",
    "start_time = time.time()\n",
    "word_to_compare = \"loneliness\"\n",
    "top_n_closest_wd_loneliness = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "word_to_compare = \"lonely\"\n",
    "top_n_closest_wd_lonely = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "word_to_compare = \"aloneness\"\n",
    "top_n_closest_wd_aloneness = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "word_to_compare = \"isolation\"\n",
    "top_n_closest_wd_isolation = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time: {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all Cosine Similarity into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word  loneliness-similarity  lonely-similarity  \\\n",
      "0       abandon               0.918922           0.898049   \n",
      "1     abandoned               0.875125           0.873156   \n",
      "2       ability               0.890194           0.876321   \n",
      "3          able               0.868866           0.857165   \n",
      "4     abortions               0.662040           0.610981   \n",
      "...         ...                    ...                ...   \n",
      "2995         yr               0.687149           0.680593   \n",
      "2996        yrs               0.624784           0.620009   \n",
      "2997       zero               0.833694           0.855695   \n",
      "2998       zone               0.834811           0.829768   \n",
      "2999       zoom               0.809533           0.804235   \n",
      "\n",
      "      aloneness-similarity  isolation-similarity  \n",
      "0                 0.774498              0.927264  \n",
      "1                 0.760706              0.886401  \n",
      "2                 0.725984              0.889708  \n",
      "3                 0.675333              0.842308  \n",
      "4                 0.678664              0.657137  \n",
      "...                    ...                   ...  \n",
      "2995              0.652395              0.654497  \n",
      "2996              0.587062              0.606020  \n",
      "2997              0.703625              0.860705  \n",
      "2998              0.715336              0.881830  \n",
      "2999              0.663190              0.784803  \n",
      "\n",
      "[3000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_top_n_closest_wd_loneliness = pd.DataFrame(top_n_closest_wd_loneliness, columns=['word', 'loneliness-similarity'])\n",
    "df_top_n_closest_wd_lonely = pd.DataFrame(top_n_closest_wd_lonely, columns=['word', 'lonely-similarity'])\n",
    "df_top_n_closest_wd_aloneness = pd.DataFrame(top_n_closest_wd_aloneness, columns=['word', 'aloneness-similarity'])\n",
    "df_top_n_closest_wd_isolation = pd.DataFrame(top_n_closest_wd_isolation, columns=['word', 'isolation-similarity'])\n",
    "\n",
    "df_top_n_closet_wd = df_top_n_closest_wd_loneliness\n",
    "df_top_n_closet_wd['lonely-similarity'] = df_top_n_closest_wd_lonely['lonely-similarity']\n",
    "df_top_n_closet_wd['aloneness-similarity'] = df_top_n_closest_wd_aloneness['aloneness-similarity']\n",
    "df_top_n_closet_wd['isolation-similarity'] = df_top_n_closest_wd_isolation['isolation-similarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calaulate the mean of Cosine Similarity for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where Average is greater than 0.75 :\n",
      "            word  loneliness-similarity  lonely-similarity  \\\n",
      "1599  loneliness               1.000000           0.931305   \n",
      "1600      lonely               0.931305           1.000000   \n",
      "1607     longing               0.936320           0.946851   \n",
      "1448   isolation               0.905714           0.890028   \n",
      "2171   rejection               0.938780           0.911769   \n",
      "...          ...                    ...                ...   \n",
      "1701       memes               0.761811           0.773769   \n",
      "2307      season               0.759253           0.773558   \n",
      "948     external               0.781475           0.755014   \n",
      "1004        fees               0.745641           0.747480   \n",
      "2007   political               0.771576           0.749205   \n",
      "\n",
      "      aloneness-similarity  isolation-similarity   Average  \n",
      "1599              0.822874              0.905714  0.914973  \n",
      "1600              0.786491              0.890028  0.901956  \n",
      "1607              0.804302              0.918827  0.901575  \n",
      "1448              0.796107              1.000000  0.897963  \n",
      "2171              0.809733              0.927427  0.896927  \n",
      "...                    ...                   ...       ...  \n",
      "1701              0.694346              0.771574  0.750375  \n",
      "2307              0.662256              0.806324  0.750348  \n",
      "948               0.678743              0.786117  0.750337  \n",
      "1004              0.691600              0.815945  0.750167  \n",
      "2007              0.684011              0.795680  0.750118  \n",
      "\n",
      "[2439 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_top_n_closet_wd['Average'] = df_top_n_closet_wd[['loneliness-similarity', 'lonely-similarity', 'aloneness-similarity','isolation-similarity']].mean(axis=1)\n",
    "df_top_n_closet_wd_sorted\n",
    "df_top_n_closet_wd_sorted.to_parquet(get_data() / 'bert_word.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Consine Similarity when ngram = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 3000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_feature=3000\n",
    "cv=CountVectorizer(max_features = size_of_feature, stop_words='english', ngram_range=(2, 2))\n",
    "X=cv.fit_transform(corpus)\n",
    "feat_dict=cv.get_feature_names_out()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 67.83481073379517\n"
     ]
    }
   ],
   "source": [
    "# Apply the get_bert_embedding function to your cleaned sentences\n",
    "start_time = time.time()\n",
    "word_to_compare = \"feel lonely\"\n",
    "\n",
    "top_n_closest_phrase = top_n_closest_words(word_to_compare, feat_dict, 2000)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJ4oOZhqEQw5eRu9A06Ngx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lonely",
   "language": "python",
   "name": "lonely"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
