{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19664,
     "status": "ok",
     "timestamp": 1689815845648,
     "user": {
      "displayName": "Raghav Aggarwal",
      "userId": "04371493702700305700"
     },
     "user_tz": 240
    },
    "id": "2bYHIJwSGEIn",
    "outputId": "b78b21e4-e020-4d44-eb1f-bd2c1530ab12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\weigfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import ssl\n",
    "import nltk\n",
    "import re\n",
    "import time \n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib \n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from utils.system import *\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# To suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"the specific warning message\", module=\"the_module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    # Correcting common abbreviations and numbers with periods to avoid incorrect splits\n",
    "    text = re.sub(r'\\b(e.g.|i.e.|etc.)\\b', lambda x: x.group().replace('.', ''), text)\n",
    "    text = re.sub(r'(\\d+)\\.', r'\\1', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize the corrected text into sentences\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Mean pooling function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Function to get word embedding using mean pooling\n",
    "def get_word_embedding(word):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        outputs = model(**inputs)\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    word_embedding = mean_pooling(outputs, attention_mask)\n",
    "    return word_embedding.squeeze().numpy()\n",
    "\n",
    "# Function to find top N closest words\n",
    "def cosine_similarity_for_words(word, word_list):\n",
    "    word_embedding = get_word_embedding(word)\n",
    "    closest_words = []\n",
    "\n",
    "    for other_word in word_list:\n",
    "        other_word_embedding = get_word_embedding(other_word)\n",
    "        similarity = 1 - cosine(word_embedding, other_word_embedding)\n",
    "        closest_words.append((other_word, similarity))\n",
    "\n",
    "    return closest_words\n",
    "\n",
    "# Function to find top N closest words\n",
    "def top_n_closest_words(word, word_list, n=5):\n",
    "    word_embedding = get_word_embedding(word)\n",
    "    closest_words = []\n",
    "\n",
    "    for other_word in word_list:\n",
    "        other_word_embedding = get_word_embedding(other_word)\n",
    "        similarity = 1 - cosine(word_embedding, other_word_embedding)\n",
    "        closest_words.append((other_word, similarity))\n",
    "\n",
    "    # Sort by similarity\n",
    "    closest_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return closest_words[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(get_data() / 'clean_data.parquet.brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Lonely Articles\n",
    "lonely_data = data.copy(deep=True)\n",
    "lonely_data = lonely_data.reset_index(drop=True)\n",
    "lonely_data['id'] = lonely_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "lonely_data['sentences'] = lonely_data['cleaned_article'].apply(split_into_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Word Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i, text in enumerate(lonely_data['sentences']):\n",
    "    post = text\n",
    "    post = ' '.join(post)\n",
    "    corpus.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cosine Similarity (Ngram = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 3000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_feature=3000\n",
    "cv=CountVectorizer(max_features = size_of_feature, stop_words='english', ngram_range=(1, 1))\n",
    "X=cv.fit_transform(corpus)\n",
    "feat_dict=cv.get_feature_names_out()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 236.8773136138916\n"
     ]
    }
   ],
   "source": [
    "# Apply the get_bert_embedding function to your cleaned sentences\n",
    "start_time = time.time()\n",
    "word_to_compare = \"loneliness\"\n",
    "top_n_closest_wd_loneliness = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "word_to_compare = \"lonely\"\n",
    "top_n_closest_wd_lonely = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "word_to_compare = \"aloneness\"\n",
    "top_n_closest_wd_aloneness = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "word_to_compare = \"isolation\"\n",
    "top_n_closest_wd_isolation = cosine_similarity_for_words(word_to_compare, feat_dict)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time: {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate and Calculate Average Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_n_closest_wd_loneliness = pd.DataFrame(top_n_closest_wd_loneliness, columns=['word', 'loneliness-similarity'])\n",
    "df_top_n_closest_wd_lonely = pd.DataFrame(top_n_closest_wd_lonely, columns=['word', 'lonely-similarity'])\n",
    "df_top_n_closest_wd_aloneness = pd.DataFrame(top_n_closest_wd_aloneness, columns=['word', 'aloneness-similarity'])\n",
    "df_top_n_closest_wd_isolation = pd.DataFrame(top_n_closest_wd_isolation, columns=['word', 'isolation-similarity'])\n",
    "\n",
    "df_top_n_closet_wd = df_top_n_closest_wd_loneliness\n",
    "df_top_n_closet_wd['lonely-similarity'] = df_top_n_closest_wd_lonely['lonely-similarity']\n",
    "df_top_n_closet_wd['aloneness-similarity'] = df_top_n_closest_wd_aloneness['aloneness-similarity']\n",
    "df_top_n_closet_wd['isolation-similarity'] = df_top_n_closest_wd_isolation['isolation-similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Data\n",
    "df_top_n_closet_wd['average'] = df_top_n_closet_wd[['loneliness-similarity', 'lonely-similarity', 'aloneness-similarity','isolation-similarity']].mean(axis=1)\n",
    "df_top_n_closet_wd_sorted.to_parquet(get_data() / 'bert_word_cosine.parquet.brotli', compression='brotli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cosine Similarity (Ngram = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7999, 3000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_feature=3000\n",
    "cv=CountVectorizer(max_features = size_of_feature, stop_words='english', ngram_range=(2, 2))\n",
    "X=cv.fit_transform(corpus)\n",
    "feat_dict=cv.get_feature_names_out()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 67.83481073379517\n"
     ]
    }
   ],
   "source": [
    "# Apply the get_bert_embedding function to your cleaned sentences\n",
    "start_time = time.time()\n",
    "word_to_compare = \"feel lonely\"\n",
    "\n",
    "top_n_closest_phrase = top_n_closest_words(word_to_compare, feat_dict, 2000)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJ4oOZhqEQw5eRu9A06Ngx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lonely",
   "language": "python",
   "name": "lonely"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
