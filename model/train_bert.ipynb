{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef689b0d-da11-487b-b34e-cf015b8b2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weigfan\\AppData\\Local\\anaconda3\\envs\\lonely\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from utils.system import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a386f0e-37b2-437e-b7ea-9908e0f0726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12983914648270708445\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14267973632\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 70117088583653821\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc8af2-bd6f-4a06-b376-d17eaad8f628",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb57e884-46a8-49dc-8b0c-2a6724cb2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(label, prediction):\n",
    "    cm = confusion_matrix(label, prediction)\n",
    "    TP = cm[1, 1]  # True Positives\n",
    "    TN = cm[0, 0]  # True Negatives\n",
    "    FP = cm[0, 1]  # False Positives\n",
    "    FN = cm[1, 0]  # False Negatives\n",
    "    \n",
    "    # Calculate precision and recall for the positive class\n",
    "    precision_pos = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall_pos = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos) if (precision_pos + recall_pos) != 0 else 0\n",
    "    \n",
    "    # Calculate precision and recall for the negative class\n",
    "    precision_neg = TN / (TN + FN) if (TN + FN) != 0 else 0\n",
    "    recall_neg = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    f1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg) if (precision_neg + recall_neg) != 0 else 0\n",
    "    \n",
    "    # Display in a table\n",
    "    metrics = pd.DataFrame({\n",
    "        'Metric': ['Precision (Positive)', 'Recall (Positive)', 'F1 Score (Positive)',\n",
    "                   'Precision (Negative)', 'Recall (Negative)', 'F1 Score (Negative)'],\n",
    "        'Value': [precision_pos, recall_pos, f1_pos, precision_neg, recall_neg, f1_neg]\n",
    "    })\n",
    "    \n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97701472-7483-4540-a8c5-8c2f627b348a",
   "metadata": {},
   "source": [
    "#### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16da8344-1fc0-4fb8-8d7a-423788cdbca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = pd.read_csv(get_data() / 'human_annotations_all_8000_overall.csv')\n",
    "art_emb = pd.read_parquet(get_data() / 'bert_article_emb.parquet.brotli')\n",
    "sentence_emb = pd.read_parquet(get_data() / 'bert_sentence_cosine.parquet.brotli')  \n",
    "sent = pd.read_parquet(get_data() / 'bert_sentiment.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a90f49-0851-42f9-9f9e-1b0098483ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent['sent_score'] = sent.apply(\n",
    "    lambda row: 1 if row['sent_article'] == 'POSITIVE' and row['conf_article'] > 0.75 else \n",
    "    (-1 if row['sent_article'] == 'NEGATIVE' and row['conf_article'] > 0.75 else 0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3702d9d4-871c-410e-89b6-b132c82f4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_emb = pd.merge(art_emb, sentence_emb, on='id', how='inner').merge(sent[['sent_score']], on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e75df778-6135-4f21-b16e-9f3a2b58ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_emb['comb_emb'] = merged_emb.apply(lambda row: [*row['bert_emb_art'], *row['bert_emb_min'], *row['bert_emb_max']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ba7b2-f91a-459d-899a-d96d7e1fab05",
   "metadata": {},
   "source": [
    "#### Out of Sample Train Model (Embedding + Sent + Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59f6464e-1f06-49c8-ad75-b239303e84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = merged_emb.sort_values('overall_label')\n",
    "df_class_0 = undersample[undersample['overall_label'] == 0]\n",
    "df_class_1 = undersample[undersample['overall_label'] == 1]\n",
    "n_samples = min(len(df_class_0), len(df_class_1))\n",
    "# Randomly sample from each class\n",
    "df_class_0_under = df_class_0.sample(n_samples)\n",
    "df_class_1_under = df_class_1.sample(n_samples)\n",
    "# Combine the two dataframes\n",
    "merged_undersample = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "# Shuffle the balanced dataset\n",
    "merged_undersample = merged_undersample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8f36f42-98ef-4a1a-ba73-f8adce4d78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_data = merged_undersample['cleaned_article']\n",
    "precomputed_embeddings_np = np.stack(merged_undersample['comb_emb'].values)\n",
    "labels = merged_undersample['overall_label']\n",
    "sent_scores_np = np.array(merged_undersample['sent_score']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38ab4af7-30a6-4ef6-8e09-9af79417dfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['distilbert.transformer.layer.5.ffn.lin1.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'pre_classifier.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'classifier.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1eaf9a13-30eb-462c-b647-5a848a9d8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer(raw_text_data.tolist(), padding=True, truncation=True, return_tensors='tf')\n",
    "input_ids = tokenized_data['input_ids']\n",
    "attention_mask = tokenized_data['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5c89f67-fd79-4605-ac38-def652ef2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_np = input_ids.numpy()\n",
    "attention_mask_np = attention_mask.numpy()\n",
    "labels_np = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d683981-bf6d-4f76-b28a-7083a3d77843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "input_ids_train, input_ids_test, attention_mask_train, attention_mask_test, embeddings_train, embeddings_test, sent_scores_train, sent_scores_test, labels_train, labels_test = train_test_split(\n",
    "    input_ids_np, attention_mask_np, precomputed_embeddings_np, sent_scores_np, labels_np, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1a5db-a0aa-4e0e-9a3a-7ce07549d435",
   "metadata": {},
   "source": [
    "##### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ded8f5eb-d214-4e40-b85e-4fcd75fd793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT input\n",
    "input_ids_layer = Input(shape=(input_ids_np.shape[1],), dtype=tf.int32, name='input_ids')\n",
    "attention_mask_layer = Input(shape=(attention_mask_np.shape[1],), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# BERT model\n",
    "bert_output = bert_model(input_ids_layer, attention_mask=attention_mask_layer)[1]\n",
    "\n",
    "# Additional features input\n",
    "sentiment_input = Input(shape=(sent_scores_np.shape[1],), name='sent_score')\n",
    "embedding_input = Input(shape=(precomputed_embeddings_np.shape[1],), name='comb_emb')\n",
    "\n",
    "# Concatenate BERT output with additional features\n",
    "concatenated = Concatenate()([bert_output, sentiment_input, embedding_input])\n",
    "\n",
    "# Dense layers\n",
    "dense = Dense(64, activation='relu')(concatenated)\n",
    "dropout = Dropout(0.5)(dense)\n",
    "batch_norm = BatchNormalization()(dropout)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(batch_norm)\n",
    "\n",
    "# Construct the model\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer, sentiment_input, embedding_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2446426e-8881-4d27-862d-ecfcf1f0af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "526/526 [==============================] - 281s 512ms/step - loss: 0.6819 - accuracy: 0.5897 - val_loss: 0.6654 - val_accuracy: 0.6111\n",
      "Epoch 2/100\n",
      "526/526 [==============================] - 268s 509ms/step - loss: 0.6474 - accuracy: 0.6268 - val_loss: 0.5995 - val_accuracy: 0.6816\n",
      "Epoch 3/100\n",
      "526/526 [==============================] - 268s 509ms/step - loss: 0.6561 - accuracy: 0.6052 - val_loss: 0.6443 - val_accuracy: 0.6453\n",
      "Epoch 4/100\n",
      "526/526 [==============================] - 268s 509ms/step - loss: 0.6532 - accuracy: 0.6166 - val_loss: 0.6355 - val_accuracy: 0.6688\n",
      "Epoch 5/100\n",
      "526/526 [==============================] - 268s 510ms/step - loss: 0.6513 - accuracy: 0.6194 - val_loss: 0.6608 - val_accuracy: 0.5983\n",
      "Epoch 6/100\n",
      "526/526 [==============================] - 269s 512ms/step - loss: 0.6561 - accuracy: 0.5959 - val_loss: 0.6507 - val_accuracy: 0.6346\n",
      "Epoch 7/100\n",
      "526/526 [==============================] - 270s 512ms/step - loss: 0.6424 - accuracy: 0.6271 - val_loss: 0.6369 - val_accuracy: 0.6154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a44a8a3400>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT \n",
    "model.fit(\n",
    "    [input_ids_train, attention_mask_train, sent_scores_train, embeddings_train], \n",
    "    labels_train, \n",
    "    epochs=100, \n",
    "    batch_size=8, \n",
    "    validation_split=0.10,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5df617af-fc03-43ec-bef0-7bdca9d5cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 24s 595ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_test = model.predict([input_ids_test, attention_mask_test, sent_scores_test, embeddings_test])\n",
    "predicted_labels_test = (predictions_test > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5fdcd990-4ff6-487f-81a4-85bc2ace1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.700000\n",
      "1     Recall (Positive)  0.775087\n",
      "2   F1 Score (Positive)  0.735632\n",
      "3  Precision (Negative)  0.754253\n",
      "4     Recall (Negative)  0.675127\n",
      "5   F1 Score (Negative)  0.712500\n"
     ]
    }
   ],
   "source": [
    "# All Embedding + Sent + Bert\n",
    "metric = get_metric(labels_test, predicted_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf28e8c-bf6c-4de3-a6bf-4ff20b454fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lonely",
   "language": "python",
   "name": "lonely"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
