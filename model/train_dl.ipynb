{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "21e0bceb-3595-4277-9a58-92e4fa17afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from keras.layers import Input, Embedding, LSTM, GRU, Dense, Dropout, Concatenate, BatchNormalization, Bidirectional, Reshape\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "from utils.system import *\n",
    "from metric import get_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "1a386f0e-37b2-437e-b7ea-9908e0f0726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10398075930866258647\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14267973632\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2640054541639576106\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68eebe4-279c-451d-8918-9ef517c11ff1",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "93711d11-08b3-4629-b515-154ea43efa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "article = pd.read_csv(get_data() / 'human_annotations_all_8000_overall.csv')\n",
    "art_emb = pd.read_parquet(get_data() / 'bert_article_emb.parquet.brotli')\n",
    "sentence_emb = pd.read_parquet(get_data() / 'bert_sentence_cosine.parquet.brotli')  \n",
    "sent = pd.read_parquet(get_data() / 'bert_sentiment.parquet.brotli')\n",
    "art_cos = pd.read_parquet(get_data() / 'bert_art_cosine.parquet.brotli')  \n",
    "emotion = pd.read_parquet(get_data() / 'bert_emotion.parquet.brotli')\n",
    "topic = pd.read_parquet(get_data() / 'lda_topic.parquet.brotli')\n",
    "n_gram = pd.read_parquet(get_data() / 'n_gram.parquet.brotli')\n",
    "lex_div = pd.read_parquet(get_data() / 'lexical_div.parquet.brotli')\n",
    "readability = pd.read_parquet(get_data() / 'readability.parquet.brotli')\n",
    "time_freq = pd.read_parquet(get_data() / 'time.parquet.brotli')\n",
    "lexicon = pd.read_parquet(get_data() / 'bert_word_cosine.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "f23b1ea6-a4d1-4823-8803-fdff743508f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data together\n",
    "merged_emb = (pd.merge(art_emb, sentence_emb, on='id', how='inner')\n",
    "              .merge(sent, on='id', how='inner')\n",
    "              .merge(art_cos, on='id', how='inner')\n",
    "              .merge(emotion, on='id', how='inner')\n",
    "              .merge(lex_div, on='id', how='inner')\n",
    "              .merge(topic, on='id', how='inner')\n",
    "              .merge(n_gram, on='id', how='inner')\n",
    "              .merge(time_freq, on='id', how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "d62892fd-4458-4367-9d6d-345b42a800b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top 1000 words\n",
    "lexicon = lexicon.head(500)\n",
    "lexicon = lexicon.reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5ce52-8dec-4e77-b61c-a6a851f6161f",
   "metadata": {},
   "source": [
    "### Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "fce28b7d-e991-439f-93af-e4f5e43a0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = merged_emb.sort_values('overall_label')\n",
    "df_class_0 = undersample[undersample['overall_label'] == 0]\n",
    "df_class_1 = undersample[undersample['overall_label'] == 1]\n",
    "n_samples = min(len(df_class_0), len(df_class_1))\n",
    "# Randomly sample from each class\n",
    "df_class_0_under = df_class_0.sample(n_samples)\n",
    "df_class_1_under = df_class_1.sample(n_samples)\n",
    "# Combine the two dataframes\n",
    "merged_undersample = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "# Shuffle the balanced dataset\n",
    "merged_undersample = merged_undersample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd6b25-d823-454b-96ab-2a350cd78905",
   "metadata": {},
   "source": [
    "### Convert Lexicon Dictionary to Numerical Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "0cb03b53-ec72-4d13-b3ad-0c8c79e31072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_features(article, lexicon):\n",
    "    features = {}\n",
    "    words = set(article.split())\n",
    "    for word in lexicon:\n",
    "        features[f'binary_{word}'] = word in words\n",
    "    return features\n",
    "\n",
    "def create_count_features(article, lexicon):\n",
    "    features = {}\n",
    "    word_counts = Counter(article.split())\n",
    "    for word in lexicon:\n",
    "        features[f'count_{word}'] = word_counts[word]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "c6e01f64-de82-423c-b1c3-50f672a0d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_list = lexicon['word'].tolist()\n",
    "# For binary features\n",
    "merged_undersample['binary_features'] = merged_undersample['cleaned_article'].apply(lambda x: create_binary_features(x, lexicon_list))\n",
    "# For count features\n",
    "merged_undersample['count_features'] = merged_undersample['cleaned_article'].apply(lambda x: create_count_features(x, lexicon_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae93a6-9096-4def-bd8a-0c33405fb1e9",
   "metadata": {},
   "source": [
    "#### Format Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "e410c0e1-fc4d-4a0a-b675-befbd51a6cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'overall_label', 'cleaned_article', 'bert_emb_art',\n",
       "       'bert_emb_min', 'bert_emb_max', 'sent_score', 'cosine_sim_art_mean',\n",
       "       'cosine_sim_0', 'cosine_sim_1', 'cosine_sim_2', 'cosine_sim_3',\n",
       "       'cosine_sim_4', 'emotion_num', 'ttr', 'Topic_0', 'Topic_1', 'Topic_2',\n",
       "       'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8',\n",
       "       'Topic_9', 'n_gram_1', 'n_gram_2', 'n_gram_3', 'n_gram_4', 'n_gram_5',\n",
       "       'n_gram_6', 'n_gram_7', 'n_gram_8', 'n_gram_9', 'n_gram_10',\n",
       "       'n_gram_11', 'n_gram_12', 'n_gram_13', 'n_gram_14', 'n_gram_15',\n",
       "       'n_gram_16', 'n_gram_17', 'n_gram_18', 'n_gram_19', 'n_gram_20',\n",
       "       'n_gram_21', 'n_gram_22', 'n_gram_23', 'n_gram_24', 'n_gram_25',\n",
       "       'n_gram_26', 'n_gram_27', 'n_gram_28', 'n_gram_29', 'n_gram_30',\n",
       "       'n_gram_31', 'n_gram_32', 'n_gram_33', 'n_gram_34', 'n_gram_35',\n",
       "       'n_gram_36', 'n_gram_37', 'n_gram_38', 'n_gram_39', 'n_gram_40',\n",
       "       'time_reference_count', 'binary_features', 'count_features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_undersample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "4769917f-57cb-4895-99a0-3b7ddcac836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_feature = pd.json_normalize(merged_undersample['count_features'])\n",
    "article_emb_feature = np.stack(merged_undersample['bert_emb_art'].to_numpy())\n",
    "max_sentence_emb_feature = np.stack(merged_undersample['bert_emb_max'].to_numpy())\n",
    "min_sentence_emb_feature = np.stack(merged_undersample['bert_emb_min'].to_numpy())\n",
    "emotion_feature = np.array(merged_undersample['emotion_num']).reshape(-1, 1)\n",
    "cosine_feature = np.array(merged_undersample['cosine_sim_art_mean']).reshape(-1, 1)\n",
    "\n",
    "label = merged_undersample['overall_label'].to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87087d65-9724-425f-a0a9-f80deb2f7d08",
   "metadata": {},
   "source": [
    "#### Out of Sample Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "a6662530-0a7b-4a38-8053-21f1d17ccad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "emb_art_train, emb_art_test, max_emb_sent_train, max_emb_sent_test, min_emb_sent_train, min_emb_sent_test, lexicon_train, lexicon_test, emotion_train, emotion_test, cosine_train, cosine_test, label_train, label_test = train_test_split(\n",
    "    article_emb_feature,\n",
    "    max_sentence_emb_feature,\n",
    "    min_sentence_emb_feature,\n",
    "    lexicon_feature,\n",
    "    emotion_feature,\n",
    "    cosine_feature,\n",
    "    label,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e2e4c-0292-4461-81e1-18f4d068e98c",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "ccdd7ca9-77d0-4118-876e-df1d8318a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(label, prediction):\n",
    "    cm = confusion_matrix(label, prediction)\n",
    "    TP = cm[1, 1]  # True Positives\n",
    "    TN = cm[0, 0]  # True Negatives\n",
    "    FP = cm[0, 1]  # False Positives\n",
    "    FN = cm[1, 0]  # False Negatives\n",
    "\n",
    "    # Calculate precision and recall for the positive class\n",
    "    precision_pos = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall_pos = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos) if (precision_pos + recall_pos) != 0 else 0\n",
    "\n",
    "    # Calculate precision and recall for the negative class\n",
    "    precision_neg = TN / (TN + FN) if (TN + FN) != 0 else 0\n",
    "    recall_neg = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    f1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg) if (precision_neg + recall_neg) != 0 else 0\n",
    "\n",
    "    # Display in a table\n",
    "    metrics = pd.DataFrame({\n",
    "        'Metric': ['Precision (Positive)', 'Recall (Positive)', 'F1 Score (Positive)',\n",
    "                   'Precision (Negative)', 'Recall (Negative)', 'F1 Score (Negative)'],\n",
    "        'Value': [precision_pos, recall_pos, f1_pos, precision_neg, recall_neg, f1_neg]\n",
    "    })\n",
    "    print(metrics)\n",
    "    return metrics\n",
    "\n",
    "def eval_result(model, feature_test, label_test):\n",
    "    predictions_test = model.predict(feature_test)\n",
    "    predicted_labels_test = (predictions_test > 0.5).astype(int)\n",
    "    metric = get_metric(label_test, predicted_labels_test)\n",
    "    return metric\n",
    "\n",
    "def create_feature(units, dropout, feature_data):\n",
    "    input_feature = Input(shape=(feature_data.shape[1],))\n",
    "    dense_feature = Dense(units, activation='relu')(input_feature)\n",
    "    dropout_feature = Dropout(dropout)(dense_feature)\n",
    "    return input_feature, dropout_feature\n",
    "    \n",
    "def train_lstm(units, dropout, l2, learn_rate, feature_train, label_train,feature_test, label_test):\n",
    "    # Create feature layer\n",
    "    inputs = []\n",
    "    features = []\n",
    "    for feature_data in feature_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # LSTM branch (Processing Article Embeddings)\n",
    "    lstm_art_input = Reshape((1, units))(features[0])\n",
    "    lstm_art = LSTM(units, dropout=dropout, recurrent_dropout=dropout)(lstm_art_input)\n",
    "\n",
    "    # LSTM branch (Processing Lexicon)\n",
    "    lstm_lexicon_input = Reshape((1, units))(features[1])\n",
    "    lstm_lexicon = LSTM(units, dropout=dropout, recurrent_dropout=dropout)(lstm_lexicon_input)\n",
    "\n",
    "    # Concatenate\n",
    "    concat_layer = Concatenate()([lstm_art, lstm_lexicon] + features[2:])\n",
    "    batch_norm = BatchNormalization()(concat_layer)\n",
    "    dense_layer = Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2))(batch_norm)\n",
    "    dropout_dense = Dropout(dropout)(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_dense)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [data for data in feature_train],\n",
    "        label_train,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate Results\n",
    "    metric = eval_result(model, feature_test, label_test)\n",
    "    return model, metric\n",
    "\n",
    "def train_gru(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test):\n",
    "    # Create feature layer\n",
    "    inputs = []\n",
    "    features = []\n",
    "    for feature_data in feature_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # GRU branch\n",
    "    gru_input = Reshape((1, units))(features[0])\n",
    "    gru_layer = GRU(units, dropout=dropout, recurrent_dropout=dropout)(gru_input)\n",
    "\n",
    "    # Concatenate\n",
    "    concat_layer = Concatenate()([gru_layer] + features[1:])\n",
    "    batch_norm = BatchNormalization()(concat_layer)\n",
    "    dense_layer = Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2))(batch_norm)\n",
    "    dropout_dense = Dropout(dropout)(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_dense)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [data for data in feature_train],\n",
    "        label_train,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate Results\n",
    "    print(\"-\"*60)\n",
    "    metric = eval_result(model, feature_test, label_test)\n",
    "    return model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "6262cd89-a201-49a8-86d7-4100427ea823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(dropoout, feature_layer):\n",
    "    # LSTM branch\n",
    "    lstm_input = Reshape((1, units))(feature_layer)\n",
    "    lstm_layer = LSTM(1, dropout=dropout, recurrent_dropout=dropout)(lstm_input) #Outputs a scalar\n",
    "    return lstm_layer\n",
    "    \n",
    "def train_lstm_perceptron(units, dropout, l2, learn_rate, lstm_train, scalar_train, label_train, lstm_test, scalar_test, label_test):\n",
    "    # Create feature layer\n",
    "    inputs = []\n",
    "    lstm_features = []\n",
    "    scalar_features = []\n",
    "    for feature_data in feature_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        lstm_features.append(feature)\n",
    "    for feature_data in scalar_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        scalar_features.append(feature)\n",
    "\n",
    "    # Create LSTM layers\n",
    "    lstm_layers = [create_lstm(dropout, feature_layer) for feature_layer in lstm_features]\n",
    "\n",
    "    # Concatenate\n",
    "    concat_layer = Concatenate()(lstm_layers + scalar_features)\n",
    "    batch_norm = BatchNormalization()(concat_layer)\n",
    "    dense_layer = Dense(units, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2))(batch_norm)\n",
    "    dropout_dense = Dropout(dropout)(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_dense)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        lstm_train + scalar_train,\n",
    "        label_train,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate Results\n",
    "    metric = eval_result(model, lstm_test + scalar_test, label_test)\n",
    "    return model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "566b51f6-e578-433a-bcff-364b215b648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "dropout = 0.5\n",
    "l2 = 0.01\n",
    "learn_rate = 0.0001\n",
    "feature_train = [emb_art_train, lexicon_train, max_emb_sent_train]\n",
    "scalar_train = [cosine_train, emotion_train]\n",
    "label_train = label_train\n",
    "feature_test = [emb_art_test, lexicon_test, max_emb_sent_test]\n",
    "scalar_test = [cosine_test, emotion_test]\n",
    "label_test = label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f304e-2f15-488a-9af2-9fef4e35539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = train_lstm(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b6b7d-10bc-42ac-ada9-82e3e0c507c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = train_gru(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "1251d9cb-0c4e-4e89-a7b6-e0aca703c4a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 10s 31ms/step - loss: 1.6469 - accuracy: 0.4941 - val_loss: 1.4350 - val_accuracy: 0.6162\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 1.4977 - accuracy: 0.5282 - val_loss: 1.3324 - val_accuracy: 0.6411\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.4037 - accuracy: 0.5375 - val_loss: 1.2401 - val_accuracy: 0.6535\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 1.3065 - accuracy: 0.5476 - val_loss: 1.1601 - val_accuracy: 0.6722\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 1.2145 - accuracy: 0.5716 - val_loss: 1.0858 - val_accuracy: 0.6701\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 1.1462 - accuracy: 0.5889 - val_loss: 1.0201 - val_accuracy: 0.6701\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.0872 - accuracy: 0.6034 - val_loss: 0.9700 - val_accuracy: 0.6929\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 1.0327 - accuracy: 0.6184 - val_loss: 0.9285 - val_accuracy: 0.7075\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.9799 - accuracy: 0.6373 - val_loss: 0.8978 - val_accuracy: 0.7012\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.9520 - accuracy: 0.6405 - val_loss: 0.8675 - val_accuracy: 0.7033\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.9223 - accuracy: 0.6511 - val_loss: 0.8443 - val_accuracy: 0.7033\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.8789 - accuracy: 0.6691 - val_loss: 0.8225 - val_accuracy: 0.7012\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.8519 - accuracy: 0.6813 - val_loss: 0.8017 - val_accuracy: 0.7012\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.8333 - accuracy: 0.6816 - val_loss: 0.7863 - val_accuracy: 0.7095\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.8218 - accuracy: 0.6837 - val_loss: 0.7712 - val_accuracy: 0.7199\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.7921 - accuracy: 0.6991 - val_loss: 0.7534 - val_accuracy: 0.7220\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.7707 - accuracy: 0.7044 - val_loss: 0.7478 - val_accuracy: 0.7241\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.7487 - accuracy: 0.7150 - val_loss: 0.7311 - val_accuracy: 0.7220\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.7443 - accuracy: 0.7067 - val_loss: 0.7197 - val_accuracy: 0.7199\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.7316 - accuracy: 0.7136 - val_loss: 0.7045 - val_accuracy: 0.7303\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.7198 - accuracy: 0.7199 - val_loss: 0.6962 - val_accuracy: 0.7303\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.6984 - accuracy: 0.7284 - val_loss: 0.6902 - val_accuracy: 0.7324\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.6850 - accuracy: 0.7348 - val_loss: 0.6789 - val_accuracy: 0.7469\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.6768 - accuracy: 0.7316 - val_loss: 0.6709 - val_accuracy: 0.7365\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.6725 - accuracy: 0.7286 - val_loss: 0.6622 - val_accuracy: 0.7407\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.6515 - accuracy: 0.7365 - val_loss: 0.6561 - val_accuracy: 0.7365\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.6460 - accuracy: 0.7411 - val_loss: 0.6469 - val_accuracy: 0.7386\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.6248 - accuracy: 0.7567 - val_loss: 0.6422 - val_accuracy: 0.7386\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.6241 - accuracy: 0.7501 - val_loss: 0.6335 - val_accuracy: 0.7448\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.6080 - accuracy: 0.7450 - val_loss: 0.6268 - val_accuracy: 0.7427\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 0.6016 - accuracy: 0.7581 - val_loss: 0.6228 - val_accuracy: 0.7448\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5949 - accuracy: 0.7496 - val_loss: 0.6238 - val_accuracy: 0.7490\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.5900 - accuracy: 0.7664 - val_loss: 0.6124 - val_accuracy: 0.7448\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.5849 - accuracy: 0.7616 - val_loss: 0.6012 - val_accuracy: 0.7469\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 0.5736 - accuracy: 0.7692 - val_loss: 0.6007 - val_accuracy: 0.7448\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.5622 - accuracy: 0.7743 - val_loss: 0.6036 - val_accuracy: 0.7427\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.5601 - accuracy: 0.7731 - val_loss: 0.5970 - val_accuracy: 0.7469\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5581 - accuracy: 0.7646 - val_loss: 0.5898 - val_accuracy: 0.7510\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5445 - accuracy: 0.7800 - val_loss: 0.5860 - val_accuracy: 0.7573\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.5386 - accuracy: 0.7763 - val_loss: 0.5833 - val_accuracy: 0.7448\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5251 - accuracy: 0.7777 - val_loss: 0.5819 - val_accuracy: 0.7490\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.5293 - accuracy: 0.7863 - val_loss: 0.5747 - val_accuracy: 0.7573\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.5087 - accuracy: 0.7902 - val_loss: 0.5770 - val_accuracy: 0.7531\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.5083 - accuracy: 0.7840 - val_loss: 0.5758 - val_accuracy: 0.7552\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.5037 - accuracy: 0.7886 - val_loss: 0.5732 - val_accuracy: 0.7490\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.5031 - accuracy: 0.7853 - val_loss: 0.5753 - val_accuracy: 0.7469\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4907 - accuracy: 0.7973 - val_loss: 0.5650 - val_accuracy: 0.7469\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4988 - accuracy: 0.7902 - val_loss: 0.5685 - val_accuracy: 0.7510\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.4892 - accuracy: 0.7982 - val_loss: 0.5622 - val_accuracy: 0.7531\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4862 - accuracy: 0.7932 - val_loss: 0.5582 - val_accuracy: 0.7427\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4726 - accuracy: 0.7982 - val_loss: 0.5630 - val_accuracy: 0.7510\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4746 - accuracy: 0.7999 - val_loss: 0.5541 - val_accuracy: 0.7448\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4797 - accuracy: 0.7987 - val_loss: 0.5572 - val_accuracy: 0.7469\n",
      "Epoch 54/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4629 - accuracy: 0.8038 - val_loss: 0.5571 - val_accuracy: 0.7448\n",
      "Epoch 55/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4526 - accuracy: 0.8089 - val_loss: 0.5570 - val_accuracy: 0.7531\n",
      "Epoch 56/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4572 - accuracy: 0.8070 - val_loss: 0.5532 - val_accuracy: 0.7510\n",
      "Epoch 57/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4527 - accuracy: 0.8125 - val_loss: 0.5551 - val_accuracy: 0.7427\n",
      "Epoch 58/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.4398 - accuracy: 0.8130 - val_loss: 0.5566 - val_accuracy: 0.7469\n",
      "Epoch 59/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 0.4531 - accuracy: 0.8061 - val_loss: 0.5520 - val_accuracy: 0.7407\n",
      "Epoch 60/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4443 - accuracy: 0.8153 - val_loss: 0.5505 - val_accuracy: 0.7407\n",
      "Epoch 61/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4336 - accuracy: 0.8178 - val_loss: 0.5558 - val_accuracy: 0.7469\n",
      "Epoch 62/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4295 - accuracy: 0.8178 - val_loss: 0.5538 - val_accuracy: 0.7469\n",
      "Epoch 63/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4169 - accuracy: 0.8312 - val_loss: 0.5536 - val_accuracy: 0.7448\n",
      "Epoch 64/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4182 - accuracy: 0.8296 - val_loss: 0.5610 - val_accuracy: 0.7407\n",
      "Epoch 65/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4198 - accuracy: 0.8257 - val_loss: 0.5555 - val_accuracy: 0.7427\n",
      "38/38 [==============================] - 1s 3ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.734234\n",
      "1     Recall (Positive)  0.809603\n",
      "2   F1 Score (Positive)  0.770079\n",
      "3  Precision (Negative)  0.786642\n",
      "4     Recall (Negative)  0.705491\n",
      "5   F1 Score (Negative)  0.743860\n"
     ]
    }
   ],
   "source": [
    "lstm_perceptron_model = train_lstm_perceptron(units, dropout, l2, learn_rate, feature_train, scalar_train,\n",
    "                                              label_train, feature_test, scalar_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7438e-7c29-4a90-8ef5-c81b3ddec888",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "5f9ecd6c-d3ab-46f4-9100-8626d059d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "dropout = 0.5\n",
    "l2 = 0.01\n",
    "learn_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "89bebbaa-924c-4204-9181-199dda6abd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1/5\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 10s 35ms/step - loss: 1.6234 - accuracy: 0.5119 - val_loss: 1.4322 - val_accuracy: 0.6162\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.4890 - accuracy: 0.5352 - val_loss: 1.3188 - val_accuracy: 0.6328\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 1.3687 - accuracy: 0.5508 - val_loss: 1.2117 - val_accuracy: 0.6701\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 1.2800 - accuracy: 0.5585 - val_loss: 1.1207 - val_accuracy: 0.6826\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.1855 - accuracy: 0.5850 - val_loss: 1.0416 - val_accuracy: 0.6846\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 1.1103 - accuracy: 0.6103 - val_loss: 0.9767 - val_accuracy: 0.6950\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 1.0558 - accuracy: 0.6147 - val_loss: 0.9260 - val_accuracy: 0.7095\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 1.0002 - accuracy: 0.6327 - val_loss: 0.8837 - val_accuracy: 0.7116\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.9474 - accuracy: 0.6435 - val_loss: 0.8453 - val_accuracy: 0.7199\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.9222 - accuracy: 0.6428 - val_loss: 0.8178 - val_accuracy: 0.7158\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.8726 - accuracy: 0.6777 - val_loss: 0.7894 - val_accuracy: 0.7199\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.8671 - accuracy: 0.6567 - val_loss: 0.7671 - val_accuracy: 0.7137\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.8225 - accuracy: 0.6873 - val_loss: 0.7488 - val_accuracy: 0.7344\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.8087 - accuracy: 0.6869 - val_loss: 0.7314 - val_accuracy: 0.7282\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7903 - accuracy: 0.6878 - val_loss: 0.7208 - val_accuracy: 0.7220\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7617 - accuracy: 0.6998 - val_loss: 0.7020 - val_accuracy: 0.7199\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7526 - accuracy: 0.7060 - val_loss: 0.6900 - val_accuracy: 0.7324\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7365 - accuracy: 0.7062 - val_loss: 0.6815 - val_accuracy: 0.7303\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.7194 - accuracy: 0.7095 - val_loss: 0.6713 - val_accuracy: 0.7344\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7071 - accuracy: 0.7169 - val_loss: 0.6635 - val_accuracy: 0.7241\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6931 - accuracy: 0.7189 - val_loss: 0.6554 - val_accuracy: 0.7324\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6859 - accuracy: 0.7305 - val_loss: 0.6482 - val_accuracy: 0.7324\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.6643 - accuracy: 0.7323 - val_loss: 0.6366 - val_accuracy: 0.7448\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6633 - accuracy: 0.7268 - val_loss: 0.6326 - val_accuracy: 0.7365\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6449 - accuracy: 0.7353 - val_loss: 0.6248 - val_accuracy: 0.7531\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6411 - accuracy: 0.7371 - val_loss: 0.6187 - val_accuracy: 0.7365\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.6223 - accuracy: 0.7498 - val_loss: 0.6153 - val_accuracy: 0.7469\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6163 - accuracy: 0.7441 - val_loss: 0.6096 - val_accuracy: 0.7386\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6074 - accuracy: 0.7567 - val_loss: 0.6031 - val_accuracy: 0.7490\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.6044 - accuracy: 0.7531 - val_loss: 0.5987 - val_accuracy: 0.7490\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.5915 - accuracy: 0.7581 - val_loss: 0.5966 - val_accuracy: 0.7573\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 0.5893 - accuracy: 0.7565 - val_loss: 0.5905 - val_accuracy: 0.7531\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.5785 - accuracy: 0.7680 - val_loss: 0.5853 - val_accuracy: 0.7490\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.5707 - accuracy: 0.7653 - val_loss: 0.5831 - val_accuracy: 0.7427\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.5548 - accuracy: 0.7701 - val_loss: 0.5823 - val_accuracy: 0.7510\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5595 - accuracy: 0.7699 - val_loss: 0.5794 - val_accuracy: 0.7573\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5484 - accuracy: 0.7699 - val_loss: 0.5761 - val_accuracy: 0.7490\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5392 - accuracy: 0.7727 - val_loss: 0.5741 - val_accuracy: 0.7593\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5339 - accuracy: 0.7777 - val_loss: 0.5706 - val_accuracy: 0.7573\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5198 - accuracy: 0.7851 - val_loss: 0.5654 - val_accuracy: 0.7407\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.5186 - accuracy: 0.7823 - val_loss: 0.5653 - val_accuracy: 0.7510\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.5148 - accuracy: 0.7881 - val_loss: 0.5633 - val_accuracy: 0.7427\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5044 - accuracy: 0.7943 - val_loss: 0.5580 - val_accuracy: 0.7427\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.5074 - accuracy: 0.7893 - val_loss: 0.5619 - val_accuracy: 0.7510\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4935 - accuracy: 0.7980 - val_loss: 0.5631 - val_accuracy: 0.7531\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.4987 - accuracy: 0.7943 - val_loss: 0.5583 - val_accuracy: 0.7490\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4831 - accuracy: 0.7918 - val_loss: 0.5544 - val_accuracy: 0.7427\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4841 - accuracy: 0.7969 - val_loss: 0.5554 - val_accuracy: 0.7510\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4847 - accuracy: 0.7916 - val_loss: 0.5484 - val_accuracy: 0.7490\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.4666 - accuracy: 0.8056 - val_loss: 0.5466 - val_accuracy: 0.7490\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4662 - accuracy: 0.8082 - val_loss: 0.5498 - val_accuracy: 0.7427\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4647 - accuracy: 0.8107 - val_loss: 0.5463 - val_accuracy: 0.7448\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4613 - accuracy: 0.8089 - val_loss: 0.5496 - val_accuracy: 0.7552\n",
      "Epoch 54/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4593 - accuracy: 0.8075 - val_loss: 0.5457 - val_accuracy: 0.7365\n",
      "Epoch 55/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4539 - accuracy: 0.8086 - val_loss: 0.5531 - val_accuracy: 0.7510\n",
      "Epoch 56/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.4492 - accuracy: 0.8153 - val_loss: 0.5537 - val_accuracy: 0.7448\n",
      "Epoch 57/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4516 - accuracy: 0.8107 - val_loss: 0.5469 - val_accuracy: 0.7407\n",
      "Epoch 58/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4350 - accuracy: 0.8275 - val_loss: 0.5427 - val_accuracy: 0.7386\n",
      "Epoch 59/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4425 - accuracy: 0.8112 - val_loss: 0.5425 - val_accuracy: 0.7448\n",
      "Epoch 60/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4290 - accuracy: 0.8225 - val_loss: 0.5501 - val_accuracy: 0.7490\n",
      "Epoch 61/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4171 - accuracy: 0.8275 - val_loss: 0.5562 - val_accuracy: 0.7448\n",
      "Epoch 62/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4173 - accuracy: 0.8273 - val_loss: 0.5521 - val_accuracy: 0.7303\n",
      "Epoch 63/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4279 - accuracy: 0.8248 - val_loss: 0.5529 - val_accuracy: 0.7344\n",
      "Epoch 64/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4128 - accuracy: 0.8285 - val_loss: 0.5548 - val_accuracy: 0.7365\n",
      "38/38 [==============================] - 1s 5ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.733884\n",
      "1     Recall (Positive)  0.781690\n",
      "2   F1 Score (Positive)  0.757033\n",
      "3  Precision (Negative)  0.793333\n",
      "4     Recall (Negative)  0.747253\n",
      "5   F1 Score (Negative)  0.769604\n",
      "Running Fold 2/5\n",
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 11s 33ms/step - loss: 1.6130 - accuracy: 0.5128 - val_loss: 1.4337 - val_accuracy: 0.6183\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 1.4886 - accuracy: 0.5241 - val_loss: 1.3334 - val_accuracy: 0.6079\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.3752 - accuracy: 0.5497 - val_loss: 1.2402 - val_accuracy: 0.6266\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.2728 - accuracy: 0.5585 - val_loss: 1.1517 - val_accuracy: 0.6390\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.1969 - accuracy: 0.5859 - val_loss: 1.0771 - val_accuracy: 0.6473\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.1342 - accuracy: 0.5997 - val_loss: 1.0174 - val_accuracy: 0.6639\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.0771 - accuracy: 0.6124 - val_loss: 0.9656 - val_accuracy: 0.6867\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 1.0204 - accuracy: 0.6283 - val_loss: 0.9228 - val_accuracy: 0.6992\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.9773 - accuracy: 0.6352 - val_loss: 0.8865 - val_accuracy: 0.7116\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.9401 - accuracy: 0.6451 - val_loss: 0.8589 - val_accuracy: 0.7158\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.9123 - accuracy: 0.6537 - val_loss: 0.8319 - val_accuracy: 0.7054\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.8683 - accuracy: 0.6730 - val_loss: 0.8091 - val_accuracy: 0.7199\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.8511 - accuracy: 0.6758 - val_loss: 0.7899 - val_accuracy: 0.7220\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.8188 - accuracy: 0.6834 - val_loss: 0.7726 - val_accuracy: 0.7158\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.8018 - accuracy: 0.6924 - val_loss: 0.7580 - val_accuracy: 0.7324\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.7806 - accuracy: 0.6989 - val_loss: 0.7471 - val_accuracy: 0.7178\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.7657 - accuracy: 0.7037 - val_loss: 0.7319 - val_accuracy: 0.7178\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.7514 - accuracy: 0.7134 - val_loss: 0.7239 - val_accuracy: 0.7178\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.7343 - accuracy: 0.7134 - val_loss: 0.7143 - val_accuracy: 0.7199\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.7231 - accuracy: 0.7159 - val_loss: 0.7117 - val_accuracy: 0.7158\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.7155 - accuracy: 0.7187 - val_loss: 0.6997 - val_accuracy: 0.7158\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6905 - accuracy: 0.7261 - val_loss: 0.6935 - val_accuracy: 0.7220\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.6770 - accuracy: 0.7337 - val_loss: 0.6856 - val_accuracy: 0.7158\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.6701 - accuracy: 0.7272 - val_loss: 0.6754 - val_accuracy: 0.7178\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.6701 - accuracy: 0.7224 - val_loss: 0.6674 - val_accuracy: 0.7220\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 0.6450 - accuracy: 0.7459 - val_loss: 0.6639 - val_accuracy: 0.7282\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.6349 - accuracy: 0.7390 - val_loss: 0.6550 - val_accuracy: 0.7261\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6305 - accuracy: 0.7454 - val_loss: 0.6502 - val_accuracy: 0.7261\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.6112 - accuracy: 0.7514 - val_loss: 0.6419 - val_accuracy: 0.7261\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.6055 - accuracy: 0.7593 - val_loss: 0.6396 - val_accuracy: 0.7303\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5979 - accuracy: 0.7556 - val_loss: 0.6339 - val_accuracy: 0.7178\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.5881 - accuracy: 0.7565 - val_loss: 0.6316 - val_accuracy: 0.7261\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.5838 - accuracy: 0.7496 - val_loss: 0.6283 - val_accuracy: 0.7324\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.5790 - accuracy: 0.7584 - val_loss: 0.6265 - val_accuracy: 0.7344\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.5615 - accuracy: 0.7680 - val_loss: 0.6175 - val_accuracy: 0.7324\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 5s 36ms/step - loss: 0.5611 - accuracy: 0.7637 - val_loss: 0.6168 - val_accuracy: 0.7303\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.5533 - accuracy: 0.7708 - val_loss: 0.6142 - val_accuracy: 0.7220\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 0.5494 - accuracy: 0.7678 - val_loss: 0.6109 - val_accuracy: 0.7178\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 0.5361 - accuracy: 0.7773 - val_loss: 0.6085 - val_accuracy: 0.7220\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 5s 36ms/step - loss: 0.5353 - accuracy: 0.7747 - val_loss: 0.6111 - val_accuracy: 0.7282\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.5286 - accuracy: 0.7777 - val_loss: 0.6033 - val_accuracy: 0.7241\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 5s 37ms/step - loss: 0.5241 - accuracy: 0.7736 - val_loss: 0.6045 - val_accuracy: 0.7261\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 5s 36ms/step - loss: 0.5154 - accuracy: 0.7851 - val_loss: 0.6047 - val_accuracy: 0.7282\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.5055 - accuracy: 0.7895 - val_loss: 0.6017 - val_accuracy: 0.7282\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 0.5068 - accuracy: 0.7842 - val_loss: 0.5986 - val_accuracy: 0.7324\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4974 - accuracy: 0.7980 - val_loss: 0.6000 - val_accuracy: 0.7324\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4978 - accuracy: 0.7934 - val_loss: 0.5993 - val_accuracy: 0.7365\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4926 - accuracy: 0.7876 - val_loss: 0.5973 - val_accuracy: 0.7241\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4854 - accuracy: 0.7966 - val_loss: 0.5929 - val_accuracy: 0.7261\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4669 - accuracy: 0.8068 - val_loss: 0.5959 - val_accuracy: 0.7282\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4719 - accuracy: 0.8024 - val_loss: 0.5959 - val_accuracy: 0.7344\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4606 - accuracy: 0.8061 - val_loss: 0.5950 - val_accuracy: 0.7344\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4653 - accuracy: 0.8031 - val_loss: 0.5950 - val_accuracy: 0.7261\n",
      "Epoch 54/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4614 - accuracy: 0.8001 - val_loss: 0.5952 - val_accuracy: 0.7324\n",
      "38/38 [==============================] - 1s 4ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.753846\n",
      "1     Recall (Positive)  0.811258\n",
      "2   F1 Score (Positive)  0.781499\n",
      "3  Precision (Negative)  0.794595\n",
      "4     Recall (Negative)  0.733777\n",
      "5   F1 Score (Negative)  0.762976\n",
      "Running Fold 3/5\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 10s 32ms/step - loss: 1.6261 - accuracy: 0.4957 - val_loss: 1.4341 - val_accuracy: 0.4896\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.4932 - accuracy: 0.5091 - val_loss: 1.3236 - val_accuracy: 0.6224\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.3866 - accuracy: 0.5128 - val_loss: 1.2352 - val_accuracy: 0.6266\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.2818 - accuracy: 0.5363 - val_loss: 1.1575 - val_accuracy: 0.6494\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.2065 - accuracy: 0.5472 - val_loss: 1.0854 - val_accuracy: 0.6494\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.1372 - accuracy: 0.5709 - val_loss: 1.0267 - val_accuracy: 0.6535\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.0710 - accuracy: 0.5933 - val_loss: 0.9714 - val_accuracy: 0.6598\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.0269 - accuracy: 0.6043 - val_loss: 0.9263 - val_accuracy: 0.6722\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.9854 - accuracy: 0.6108 - val_loss: 0.8835 - val_accuracy: 0.6929\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.9351 - accuracy: 0.6428 - val_loss: 0.8511 - val_accuracy: 0.6971\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.9060 - accuracy: 0.6486 - val_loss: 0.8230 - val_accuracy: 0.7075\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.8795 - accuracy: 0.6525 - val_loss: 0.7965 - val_accuracy: 0.7220\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.8439 - accuracy: 0.6652 - val_loss: 0.7708 - val_accuracy: 0.7116\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.8323 - accuracy: 0.6664 - val_loss: 0.7520 - val_accuracy: 0.7158\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7914 - accuracy: 0.6910 - val_loss: 0.7349 - val_accuracy: 0.7282\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7970 - accuracy: 0.6749 - val_loss: 0.7198 - val_accuracy: 0.7158\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.7713 - accuracy: 0.6885 - val_loss: 0.7052 - val_accuracy: 0.7324\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.7481 - accuracy: 0.7116 - val_loss: 0.7001 - val_accuracy: 0.7178\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.7248 - accuracy: 0.7189 - val_loss: 0.6845 - val_accuracy: 0.7324\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 0.7147 - accuracy: 0.7210 - val_loss: 0.6758 - val_accuracy: 0.7344\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.6968 - accuracy: 0.7302 - val_loss: 0.6621 - val_accuracy: 0.7469\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 5s 37ms/step - loss: 0.6866 - accuracy: 0.7291 - val_loss: 0.6599 - val_accuracy: 0.7448\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 0.6681 - accuracy: 0.7362 - val_loss: 0.6504 - val_accuracy: 0.7303\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.6680 - accuracy: 0.7348 - val_loss: 0.6456 - val_accuracy: 0.7407\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 5s 35ms/step - loss: 0.6583 - accuracy: 0.7397 - val_loss: 0.6367 - val_accuracy: 0.7344\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 5s 36ms/step - loss: 0.6471 - accuracy: 0.7399 - val_loss: 0.6275 - val_accuracy: 0.7469\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.6342 - accuracy: 0.7369 - val_loss: 0.6208 - val_accuracy: 0.7552\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.6262 - accuracy: 0.7528 - val_loss: 0.6196 - val_accuracy: 0.7510\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6148 - accuracy: 0.7498 - val_loss: 0.6147 - val_accuracy: 0.7344\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6066 - accuracy: 0.7528 - val_loss: 0.6096 - val_accuracy: 0.7386\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5928 - accuracy: 0.7616 - val_loss: 0.6001 - val_accuracy: 0.7365\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5827 - accuracy: 0.7627 - val_loss: 0.5952 - val_accuracy: 0.7386\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.5789 - accuracy: 0.7634 - val_loss: 0.6017 - val_accuracy: 0.7365\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.5712 - accuracy: 0.7733 - val_loss: 0.5887 - val_accuracy: 0.7386\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.5684 - accuracy: 0.7648 - val_loss: 0.5895 - val_accuracy: 0.7427\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.5673 - accuracy: 0.7623 - val_loss: 0.5803 - val_accuracy: 0.7469\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.5537 - accuracy: 0.7662 - val_loss: 0.5770 - val_accuracy: 0.7386\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5401 - accuracy: 0.7784 - val_loss: 0.5754 - val_accuracy: 0.7407\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.5362 - accuracy: 0.7759 - val_loss: 0.5759 - val_accuracy: 0.7365\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.5282 - accuracy: 0.7823 - val_loss: 0.5750 - val_accuracy: 0.7407\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.5271 - accuracy: 0.7793 - val_loss: 0.5646 - val_accuracy: 0.7407\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.5202 - accuracy: 0.7879 - val_loss: 0.5643 - val_accuracy: 0.7365\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.5155 - accuracy: 0.7856 - val_loss: 0.5650 - val_accuracy: 0.7407\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4989 - accuracy: 0.7953 - val_loss: 0.5588 - val_accuracy: 0.7510\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.5044 - accuracy: 0.7853 - val_loss: 0.5575 - val_accuracy: 0.7407\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4885 - accuracy: 0.8010 - val_loss: 0.5566 - val_accuracy: 0.7448\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4922 - accuracy: 0.7982 - val_loss: 0.5641 - val_accuracy: 0.7407\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4877 - accuracy: 0.7989 - val_loss: 0.5583 - val_accuracy: 0.7448\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.4844 - accuracy: 0.8006 - val_loss: 0.5534 - val_accuracy: 0.7510\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.4782 - accuracy: 0.8015 - val_loss: 0.5534 - val_accuracy: 0.7303\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4800 - accuracy: 0.7953 - val_loss: 0.5538 - val_accuracy: 0.7386\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4680 - accuracy: 0.7976 - val_loss: 0.5523 - val_accuracy: 0.7303\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.4661 - accuracy: 0.8079 - val_loss: 0.5574 - val_accuracy: 0.7303\n",
      "Epoch 54/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4527 - accuracy: 0.8185 - val_loss: 0.5524 - val_accuracy: 0.7303\n",
      "Epoch 55/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.4480 - accuracy: 0.8185 - val_loss: 0.5512 - val_accuracy: 0.7282\n",
      "Epoch 56/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4401 - accuracy: 0.8259 - val_loss: 0.5488 - val_accuracy: 0.7303\n",
      "Epoch 57/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4399 - accuracy: 0.8165 - val_loss: 0.5579 - val_accuracy: 0.7407\n",
      "Epoch 58/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.4485 - accuracy: 0.8132 - val_loss: 0.5463 - val_accuracy: 0.7448\n",
      "Epoch 59/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 0.4275 - accuracy: 0.8238 - val_loss: 0.5487 - val_accuracy: 0.7490\n",
      "Epoch 60/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4417 - accuracy: 0.8109 - val_loss: 0.5479 - val_accuracy: 0.7344\n",
      "Epoch 61/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4375 - accuracy: 0.8234 - val_loss: 0.5518 - val_accuracy: 0.7365\n",
      "Epoch 62/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4191 - accuracy: 0.8358 - val_loss: 0.5485 - val_accuracy: 0.7386\n",
      "Epoch 63/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4217 - accuracy: 0.8245 - val_loss: 0.5523 - val_accuracy: 0.7386\n",
      "38/38 [==============================] - 1s 4ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.693038\n",
      "1     Recall (Positive)  0.764398\n",
      "2   F1 Score (Positive)  0.726971\n",
      "3  Precision (Negative)  0.764398\n",
      "4     Recall (Negative)  0.693038\n",
      "5   F1 Score (Negative)  0.726971\n",
      "Running Fold 4/5\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 11s 33ms/step - loss: 1.6116 - accuracy: 0.5063 - val_loss: 1.4366 - val_accuracy: 0.7033\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.4574 - accuracy: 0.5218 - val_loss: 1.3101 - val_accuracy: 0.6618\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.3592 - accuracy: 0.5266 - val_loss: 1.2011 - val_accuracy: 0.6680\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 1.2321 - accuracy: 0.5651 - val_loss: 1.1006 - val_accuracy: 0.6680\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 1.1598 - accuracy: 0.5723 - val_loss: 1.0239 - val_accuracy: 0.6888\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 1.0743 - accuracy: 0.6039 - val_loss: 0.9600 - val_accuracy: 0.7033\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 1.0264 - accuracy: 0.6122 - val_loss: 0.9071 - val_accuracy: 0.7075\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.9683 - accuracy: 0.6267 - val_loss: 0.8596 - val_accuracy: 0.7095\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.9239 - accuracy: 0.6362 - val_loss: 0.8230 - val_accuracy: 0.7137\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.8933 - accuracy: 0.6435 - val_loss: 0.7923 - val_accuracy: 0.7199\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.8457 - accuracy: 0.6756 - val_loss: 0.7650 - val_accuracy: 0.7303\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.8181 - accuracy: 0.6730 - val_loss: 0.7434 - val_accuracy: 0.7199\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.8063 - accuracy: 0.6779 - val_loss: 0.7311 - val_accuracy: 0.7199\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.7816 - accuracy: 0.6846 - val_loss: 0.7085 - val_accuracy: 0.7261\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7682 - accuracy: 0.6823 - val_loss: 0.7013 - val_accuracy: 0.7386\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.7519 - accuracy: 0.6926 - val_loss: 0.6803 - val_accuracy: 0.7427\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.7301 - accuracy: 0.6982 - val_loss: 0.6677 - val_accuracy: 0.7407\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.7223 - accuracy: 0.6966 - val_loss: 0.6625 - val_accuracy: 0.7386\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.7039 - accuracy: 0.7152 - val_loss: 0.6500 - val_accuracy: 0.7386\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.6881 - accuracy: 0.7127 - val_loss: 0.6407 - val_accuracy: 0.7469\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6765 - accuracy: 0.7152 - val_loss: 0.6341 - val_accuracy: 0.7552\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6577 - accuracy: 0.7316 - val_loss: 0.6235 - val_accuracy: 0.7614\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6641 - accuracy: 0.7203 - val_loss: 0.6167 - val_accuracy: 0.7365\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6441 - accuracy: 0.7344 - val_loss: 0.6154 - val_accuracy: 0.7614\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.6325 - accuracy: 0.7332 - val_loss: 0.6035 - val_accuracy: 0.7614\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.6147 - accuracy: 0.7519 - val_loss: 0.5967 - val_accuracy: 0.7552\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 5s 36ms/step - loss: 0.6165 - accuracy: 0.7399 - val_loss: 0.5926 - val_accuracy: 0.7656\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.6097 - accuracy: 0.7478 - val_loss: 0.5837 - val_accuracy: 0.7573\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 5s 37ms/step - loss: 0.6021 - accuracy: 0.7540 - val_loss: 0.5837 - val_accuracy: 0.7593\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.5935 - accuracy: 0.7489 - val_loss: 0.5750 - val_accuracy: 0.7635\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.5854 - accuracy: 0.7471 - val_loss: 0.5730 - val_accuracy: 0.7573\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 0.5859 - accuracy: 0.7519 - val_loss: 0.5653 - val_accuracy: 0.7676\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 0.5686 - accuracy: 0.7549 - val_loss: 0.5640 - val_accuracy: 0.7656\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.5636 - accuracy: 0.7600 - val_loss: 0.5637 - val_accuracy: 0.7573\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 5s 33ms/step - loss: 0.5653 - accuracy: 0.7588 - val_loss: 0.5557 - val_accuracy: 0.7552\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.5419 - accuracy: 0.7690 - val_loss: 0.5512 - val_accuracy: 0.7614\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.5478 - accuracy: 0.7625 - val_loss: 0.5484 - val_accuracy: 0.7697\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.5402 - accuracy: 0.7699 - val_loss: 0.5443 - val_accuracy: 0.7552\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.5243 - accuracy: 0.7782 - val_loss: 0.5399 - val_accuracy: 0.7759\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.5178 - accuracy: 0.7883 - val_loss: 0.5404 - val_accuracy: 0.7656\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5155 - accuracy: 0.7768 - val_loss: 0.5427 - val_accuracy: 0.7531\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.5136 - accuracy: 0.7740 - val_loss: 0.5349 - val_accuracy: 0.7614\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4996 - accuracy: 0.7883 - val_loss: 0.5340 - val_accuracy: 0.7552\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4944 - accuracy: 0.7856 - val_loss: 0.5311 - val_accuracy: 0.7573\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4996 - accuracy: 0.7860 - val_loss: 0.5312 - val_accuracy: 0.7739\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.4909 - accuracy: 0.7881 - val_loss: 0.5329 - val_accuracy: 0.7718\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4888 - accuracy: 0.7876 - val_loss: 0.5295 - val_accuracy: 0.7635\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.4860 - accuracy: 0.7941 - val_loss: 0.5268 - val_accuracy: 0.7635\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4737 - accuracy: 0.7971 - val_loss: 0.5228 - val_accuracy: 0.7614\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4832 - accuracy: 0.7899 - val_loss: 0.5237 - val_accuracy: 0.7656\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4701 - accuracy: 0.7948 - val_loss: 0.5232 - val_accuracy: 0.7697\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4755 - accuracy: 0.7992 - val_loss: 0.5191 - val_accuracy: 0.7718\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.4534 - accuracy: 0.8047 - val_loss: 0.5194 - val_accuracy: 0.7656\n",
      "Epoch 54/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 0.4505 - accuracy: 0.8036 - val_loss: 0.5303 - val_accuracy: 0.7573\n",
      "Epoch 55/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4477 - accuracy: 0.8172 - val_loss: 0.5244 - val_accuracy: 0.7490\n",
      "Epoch 56/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4500 - accuracy: 0.8142 - val_loss: 0.5179 - val_accuracy: 0.7697\n",
      "Epoch 57/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.4376 - accuracy: 0.8160 - val_loss: 0.5223 - val_accuracy: 0.7676\n",
      "Epoch 58/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4371 - accuracy: 0.8197 - val_loss: 0.5209 - val_accuracy: 0.7593\n",
      "Epoch 59/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4484 - accuracy: 0.8045 - val_loss: 0.5186 - val_accuracy: 0.7593\n",
      "Epoch 60/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 0.4331 - accuracy: 0.8153 - val_loss: 0.5199 - val_accuracy: 0.7573\n",
      "Epoch 61/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4270 - accuracy: 0.8213 - val_loss: 0.5207 - val_accuracy: 0.7676\n",
      "38/38 [==============================] - 1s 4ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.768473\n",
      "1     Recall (Positive)  0.751204\n",
      "2   F1 Score (Positive)  0.759740\n",
      "3  Precision (Negative)  0.739933\n",
      "4     Recall (Negative)  0.757732\n",
      "5   F1 Score (Negative)  0.748727\n",
      "Running Fold 5/5\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 10s 32ms/step - loss: 1.5796 - accuracy: 0.5415 - val_loss: 1.4393 - val_accuracy: 0.5353\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.4570 - accuracy: 0.5473 - val_loss: 1.3116 - val_accuracy: 0.6183\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.3594 - accuracy: 0.5403 - val_loss: 1.2174 - val_accuracy: 0.6494\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 1.2535 - accuracy: 0.5613 - val_loss: 1.1349 - val_accuracy: 0.6598\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 4s 27ms/step - loss: 1.1693 - accuracy: 0.5807 - val_loss: 1.0656 - val_accuracy: 0.6577\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 6s 42ms/step - loss: 1.1128 - accuracy: 0.5906 - val_loss: 1.0082 - val_accuracy: 0.6722\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 6s 41ms/step - loss: 1.0591 - accuracy: 0.6070 - val_loss: 0.9561 - val_accuracy: 0.6888\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.9947 - accuracy: 0.6358 - val_loss: 0.9132 - val_accuracy: 0.6846\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.9535 - accuracy: 0.6429 - val_loss: 0.8794 - val_accuracy: 0.6909\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 3s 21ms/step - loss: 0.9170 - accuracy: 0.6521 - val_loss: 0.8488 - val_accuracy: 0.7033\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 3s 21ms/step - loss: 0.8820 - accuracy: 0.6662 - val_loss: 0.8244 - val_accuracy: 0.7137\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 3s 22ms/step - loss: 0.8519 - accuracy: 0.6685 - val_loss: 0.8024 - val_accuracy: 0.7075\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.8302 - accuracy: 0.6787 - val_loss: 0.7825 - val_accuracy: 0.7095\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.7996 - accuracy: 0.6939 - val_loss: 0.7687 - val_accuracy: 0.7178\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.7828 - accuracy: 0.6929 - val_loss: 0.7509 - val_accuracy: 0.7199\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.7678 - accuracy: 0.6976 - val_loss: 0.7405 - val_accuracy: 0.7220\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 3s 21ms/step - loss: 0.7453 - accuracy: 0.7139 - val_loss: 0.7260 - val_accuracy: 0.7199\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 3s 22ms/step - loss: 0.7378 - accuracy: 0.7089 - val_loss: 0.7163 - val_accuracy: 0.7303\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.7166 - accuracy: 0.7195 - val_loss: 0.7105 - val_accuracy: 0.7199\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.7020 - accuracy: 0.7245 - val_loss: 0.6963 - val_accuracy: 0.7261\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.6907 - accuracy: 0.7241 - val_loss: 0.6889 - val_accuracy: 0.7282\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.6696 - accuracy: 0.7370 - val_loss: 0.6790 - val_accuracy: 0.7282\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.6579 - accuracy: 0.7409 - val_loss: 0.6741 - val_accuracy: 0.7241\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.6597 - accuracy: 0.7349 - val_loss: 0.6640 - val_accuracy: 0.7324\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.6442 - accuracy: 0.7425 - val_loss: 0.6621 - val_accuracy: 0.7261\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.6359 - accuracy: 0.7416 - val_loss: 0.6501 - val_accuracy: 0.7344\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.6240 - accuracy: 0.7400 - val_loss: 0.6465 - val_accuracy: 0.7220\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.6201 - accuracy: 0.7499 - val_loss: 0.6390 - val_accuracy: 0.7220\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.6051 - accuracy: 0.7510 - val_loss: 0.6306 - val_accuracy: 0.7137\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.6033 - accuracy: 0.7508 - val_loss: 0.6262 - val_accuracy: 0.7261\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.5900 - accuracy: 0.7543 - val_loss: 0.6206 - val_accuracy: 0.7386\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 3s 22ms/step - loss: 0.5790 - accuracy: 0.7639 - val_loss: 0.6164 - val_accuracy: 0.7261\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.5728 - accuracy: 0.7658 - val_loss: 0.6091 - val_accuracy: 0.7199\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 3s 23ms/step - loss: 0.5689 - accuracy: 0.7709 - val_loss: 0.6070 - val_accuracy: 0.7178\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 3s 22ms/step - loss: 0.5554 - accuracy: 0.7697 - val_loss: 0.6034 - val_accuracy: 0.7303\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 3s 22ms/step - loss: 0.5565 - accuracy: 0.7644 - val_loss: 0.5997 - val_accuracy: 0.7344\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 3s 25ms/step - loss: 0.5500 - accuracy: 0.7692 - val_loss: 0.6044 - val_accuracy: 0.7427\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 3s 24ms/step - loss: 0.5410 - accuracy: 0.7743 - val_loss: 0.5968 - val_accuracy: 0.7324\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.5355 - accuracy: 0.7794 - val_loss: 0.5922 - val_accuracy: 0.7282\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.5328 - accuracy: 0.7810 - val_loss: 0.5869 - val_accuracy: 0.7344\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.5199 - accuracy: 0.7796 - val_loss: 0.5907 - val_accuracy: 0.7282\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 5s 36ms/step - loss: 0.5192 - accuracy: 0.7799 - val_loss: 0.5915 - val_accuracy: 0.7324\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 0.5098 - accuracy: 0.7911 - val_loss: 0.5830 - val_accuracy: 0.7303\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.5105 - accuracy: 0.7858 - val_loss: 0.5824 - val_accuracy: 0.7241\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.5020 - accuracy: 0.7895 - val_loss: 0.5803 - val_accuracy: 0.7220\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 4s 32ms/step - loss: 0.4997 - accuracy: 0.7849 - val_loss: 0.5750 - val_accuracy: 0.7282\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4906 - accuracy: 0.7935 - val_loss: 0.5799 - val_accuracy: 0.7365\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4896 - accuracy: 0.7928 - val_loss: 0.5737 - val_accuracy: 0.7365\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.4913 - accuracy: 0.7905 - val_loss: 0.5751 - val_accuracy: 0.7241\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 0.4898 - accuracy: 0.7898 - val_loss: 0.5764 - val_accuracy: 0.7241\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 5s 34ms/step - loss: 0.4743 - accuracy: 0.7983 - val_loss: 0.5729 - val_accuracy: 0.7303\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 4s 33ms/step - loss: 0.4722 - accuracy: 0.7992 - val_loss: 0.5705 - val_accuracy: 0.7303\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4631 - accuracy: 0.8018 - val_loss: 0.5718 - val_accuracy: 0.7344\n",
      "Epoch 54/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4653 - accuracy: 0.7994 - val_loss: 0.5696 - val_accuracy: 0.7427\n",
      "Epoch 55/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4668 - accuracy: 0.8036 - val_loss: 0.5674 - val_accuracy: 0.7282\n",
      "Epoch 56/1000\n",
      "136/136 [==============================] - 4s 30ms/step - loss: 0.4616 - accuracy: 0.7971 - val_loss: 0.5676 - val_accuracy: 0.7365\n",
      "Epoch 57/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4558 - accuracy: 0.8064 - val_loss: 0.5682 - val_accuracy: 0.7386\n",
      "Epoch 58/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4607 - accuracy: 0.7988 - val_loss: 0.5674 - val_accuracy: 0.7365\n",
      "Epoch 59/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4416 - accuracy: 0.8084 - val_loss: 0.5721 - val_accuracy: 0.7324\n",
      "Epoch 60/1000\n",
      "136/136 [==============================] - 4s 29ms/step - loss: 0.4388 - accuracy: 0.8156 - val_loss: 0.5648 - val_accuracy: 0.7386\n",
      "Epoch 61/1000\n",
      "136/136 [==============================] - 4s 26ms/step - loss: 0.4421 - accuracy: 0.8087 - val_loss: 0.5674 - val_accuracy: 0.7386\n",
      "Epoch 62/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 0.4313 - accuracy: 0.8167 - val_loss: 0.5692 - val_accuracy: 0.7407\n",
      "Epoch 63/1000\n",
      "136/136 [==============================] - 4s 28ms/step - loss: 0.4291 - accuracy: 0.8177 - val_loss: 0.5718 - val_accuracy: 0.7427\n",
      "Epoch 64/1000\n",
      "136/136 [==============================] - 4s 31ms/step - loss: 0.4281 - accuracy: 0.8186 - val_loss: 0.5793 - val_accuracy: 0.7344\n",
      "Epoch 65/1000\n",
      "136/136 [==============================] - 3s 26ms/step - loss: 0.4245 - accuracy: 0.8213 - val_loss: 0.5732 - val_accuracy: 0.7282\n",
      "38/38 [==============================] - 1s 5ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.755556\n",
      "1     Recall (Positive)  0.791925\n",
      "2   F1 Score (Positive)  0.773313\n",
      "3  Precision (Negative)  0.746692\n",
      "4     Recall (Negative)  0.705357\n",
      "5   F1 Score (Negative)  0.725436\n",
      "Total Time: 1245.1531689167023 seconds\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "collect_metric = []\n",
    "X = range(len(merged_undersample))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Running Fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Splitting each feature into training and validation sets\n",
    "    lexicon_train, lexicon_val = lexicon_feature.iloc[train_idx], lexicon_feature.iloc[val_idx]\n",
    "    article_emb_train, article_emb_val = article_emb_feature[train_idx], article_emb_feature[val_idx]\n",
    "    max_sentence_emb_train, max_sentence_emb_val = max_sentence_emb_feature[train_idx], max_sentence_emb_feature[val_idx]\n",
    "    min_sentence_emb_train, min_sentence_emb_val = min_sentence_emb_feature[train_idx], min_sentence_emb_feature[val_idx]\n",
    "    emotion_train, emotion_val = emotion_feature[train_idx], emotion_feature[val_idx]\n",
    "    cosine_train, cosine_val = cosine_feature[train_idx], cosine_feature[val_idx]\n",
    "    label_train, label_val = label[train_idx], label[val_idx]\n",
    "\n",
    "    # Combine individual feature sets for training and validation\n",
    "    feature_train = [article_emb_train, lexicon_train, max_sentence_emb_train]\n",
    "    scalar_train = [cosine_train, emotion_train]\n",
    "    feature_val = [article_emb_val, lexicon_val, max_sentence_emb_val]\n",
    "    scalar_val = [cosine_val, emotion_val]\n",
    "    \n",
    "    # Train the model\n",
    "    # lstm_model, metric = train_lstm(units, dropout, l2, learn_rate, feature_train, label_train, feature_val, label_val)\n",
    "    lstm_perceptron, metric = train_lstm_perceptron(units, dropout, l2, learn_rate, feature_train, scalar_train, label_train,\n",
    "                                                    feature_val, scalar_val, label_val)\n",
    "    collect_metric.append(metric)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "92eee568-fcc2-4795-a811-c6eaa57c5f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.740959</td>\n",
       "      <td>Precision (Positive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780095</td>\n",
       "      <td>Recall (Positive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.759711</td>\n",
       "      <td>F1 Score (Positive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.767790</td>\n",
       "      <td>Precision (Negative)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.727431</td>\n",
       "      <td>Recall (Negative)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.746743</td>\n",
       "      <td>F1 Score (Negative)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Value                Metric\n",
       "0  0.740959  Precision (Positive)\n",
       "1  0.780095     Recall (Positive)\n",
       "2  0.759711   F1 Score (Positive)\n",
       "3  0.767790  Precision (Negative)\n",
       "4  0.727431     Recall (Negative)\n",
       "5  0.746743   F1 Score (Negative)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_metric = pd.concat(collect_metric, axis=0)\n",
    "numeric_cols = comb_metric.select_dtypes(include='number')\n",
    "final_metric = numeric_cols.groupby(numeric_cols.index).mean()\n",
    "final_metric['Metric'] = collect_metric[0]['Metric']\n",
    "final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c686c8-6557-4208-8174-955da5237648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lonely",
   "language": "python",
   "name": "lonely"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
