{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21e0bceb-3595-4277-9a58-92e4fa17afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from keras.layers import Input, Embedding, LSTM, GRU, Dense, Dropout, Concatenate, BatchNormalization, Bidirectional, Reshape\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "from utils.system import *\n",
    "from metric import get_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a386f0e-37b2-437e-b7ea-9908e0f0726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14829590517087378307\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14259585024\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14826139451116061990\n",
      "physical_device_desc: \"device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14259585024\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14371716600925789295\n",
      "physical_device_desc: \"device: 1, name: NVIDIA RTX A4000, pci bus id: 0000:b3:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 2144165316\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68eebe4-279c-451d-8918-9ef517c11ff1",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93711d11-08b3-4629-b515-154ea43efa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "article = pd.read_csv(get_data() / 'human_annotations_all_8000_overall.csv')\n",
    "art_emb = pd.read_parquet(get_data() / 'bert_article_emb.parquet.brotli')\n",
    "sentence_emb = pd.read_parquet(get_data() / 'bert_sentence_cosine.parquet.brotli')  \n",
    "sent = pd.read_parquet(get_data() / 'bert_sentiment.parquet.brotli')\n",
    "art_cos = pd.read_parquet(get_data() / 'bert_art_cosine.parquet.brotli')  \n",
    "emotion = pd.read_parquet(get_data() / 'bert_emotion.parquet.brotli')\n",
    "topic = pd.read_parquet(get_data() / 'lda_topic.parquet.brotli')\n",
    "n_gram = pd.read_parquet(get_data() / 'n_gram.parquet.brotli')\n",
    "lex_div = pd.read_parquet(get_data() / 'lexical_div.parquet.brotli')\n",
    "readability = pd.read_parquet(get_data() / 'readability.parquet.brotli')\n",
    "time = pd.read_parquet(get_data() / 'time.parquet.brotli')\n",
    "lexicon = pd.read_parquet(get_data() / 'bert_word_cosine.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f23b1ea6-a4d1-4823-8803-fdff743508f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data together\n",
    "merged_emb = (pd.merge(art_emb, sentence_emb, on='id', how='inner')\n",
    "              .merge(sent, on='id', how='inner')\n",
    "              .merge(art_cos, on='id', how='inner')\n",
    "              .merge(emotion, on='id', how='inner')\n",
    "              .merge(lex_div, on='id', how='inner')\n",
    "              .merge(topic, on='id', how='inner')\n",
    "              .merge(n_gram, on='id', how='inner')\n",
    "              .merge(time, on='id', how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62892fd-4458-4367-9d6d-345b42a800b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top 1000 words\n",
    "lexicon = lexicon.head(1500)\n",
    "lexicon = lexicon.reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5ce52-8dec-4e77-b61c-a6a851f6161f",
   "metadata": {},
   "source": [
    "### Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fce28b7d-e991-439f-93af-e4f5e43a0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = merged_emb.sort_values('overall_label')\n",
    "df_class_0 = undersample[undersample['overall_label'] == 0]\n",
    "df_class_1 = undersample[undersample['overall_label'] == 1]\n",
    "n_samples = min(len(df_class_0), len(df_class_1))\n",
    "# Randomly sample from each class\n",
    "df_class_0_under = df_class_0.sample(n_samples)\n",
    "df_class_1_under = df_class_1.sample(n_samples)\n",
    "# Combine the two dataframes\n",
    "merged_undersample = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "# Shuffle the balanced dataset\n",
    "merged_undersample = merged_undersample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd6b25-d823-454b-96ab-2a350cd78905",
   "metadata": {},
   "source": [
    "### Convert Lexicon Dictionary to Numerical Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cb03b53-ec72-4d13-b3ad-0c8c79e31072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_features(article, lexicon):\n",
    "    features = {}\n",
    "    words = set(article.split())\n",
    "    for word in lexicon:\n",
    "        features[f'binary_{word}'] = word in words\n",
    "    return features\n",
    "\n",
    "def create_count_features(article, lexicon):\n",
    "    features = {}\n",
    "    word_counts = Counter(article.split())\n",
    "    for word in lexicon:\n",
    "        features[f'count_{word}'] = word_counts[word]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6e01f64-de82-423c-b1c3-50f672a0d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_list = lexicon['word'].tolist()\n",
    "# For binary features\n",
    "merged_undersample['binary_features'] = merged_undersample['cleaned_article'].apply(lambda x: create_binary_features(x, lexicon_list))\n",
    "# For count features\n",
    "merged_undersample['count_features'] = merged_undersample['cleaned_article'].apply(lambda x: create_count_features(x, lexicon_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae93a6-9096-4def-bd8a-0c33405fb1e9",
   "metadata": {},
   "source": [
    "#### Format Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e410c0e1-fc4d-4a0a-b675-befbd51a6cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'overall_label', 'cleaned_article', 'bert_emb_art',\n",
       "       'bert_emb_min', 'bert_emb_max', 'sent_score', 'cosine_sim_art_mean',\n",
       "       'cosine_sim_0', 'cosine_sim_1', 'cosine_sim_2', 'cosine_sim_3',\n",
       "       'cosine_sim_4', 'emotion_num', 'ttr', 'Topic_0', 'Topic_1', 'Topic_2',\n",
       "       'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8',\n",
       "       'Topic_9', 'n_gram_1', 'n_gram_2', 'n_gram_3', 'n_gram_4', 'n_gram_5',\n",
       "       'n_gram_6', 'n_gram_7', 'n_gram_8', 'n_gram_9', 'n_gram_10',\n",
       "       'n_gram_11', 'n_gram_12', 'n_gram_13', 'n_gram_14', 'n_gram_15',\n",
       "       'n_gram_16', 'n_gram_17', 'n_gram_18', 'n_gram_19', 'n_gram_20',\n",
       "       'n_gram_21', 'n_gram_22', 'n_gram_23', 'n_gram_24', 'n_gram_25',\n",
       "       'n_gram_26', 'n_gram_27', 'n_gram_28', 'n_gram_29', 'n_gram_30',\n",
       "       'n_gram_31', 'n_gram_32', 'n_gram_33', 'n_gram_34', 'n_gram_35',\n",
       "       'n_gram_36', 'n_gram_37', 'n_gram_38', 'n_gram_39', 'n_gram_40',\n",
       "       'time_reference_count', 'binary_features', 'count_features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_undersample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4769917f-57cb-4895-99a0-3b7ddcac836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_feature = pd.json_normalize(merged_undersample['count_features'])\n",
    "article_emb_feature = np.stack(merged_undersample['bert_emb_art'].to_numpy())\n",
    "max_sentence_emb_feature = np.stack(merged_undersample['bert_emb_max'].to_numpy())\n",
    "min_sentence_emb_feature = np.stack(merged_undersample['bert_emb_min'].to_numpy())\n",
    "emotion_feature = np.array(merged_undersample['emotion_num']).reshape(-1, 1)\n",
    "cosine_feature = np.array(merged_undersample['cosine_sim_art_mean']).reshape(-1, 1)\n",
    "\n",
    "label = merged_undersample['overall_label'].to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87087d65-9724-425f-a0a9-f80deb2f7d08",
   "metadata": {},
   "source": [
    "#### Out of Sample Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6662530-0a7b-4a38-8053-21f1d17ccad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "emb_art_train, emb_art_test, max_emb_sent_train, max_emb_sent_test, min_emb_sent_train, min_emb_sent_test, lexicon_train, lexicon_test, emotion_train, emotion_test, cosine_train, cosine_test, label_train, label_test = train_test_split(\n",
    "    article_emb_feature,\n",
    "    max_sentence_emb_feature,\n",
    "    min_sentence_emb_feature,\n",
    "    lexicon_feature,\n",
    "    emotion_feature,\n",
    "    cosine_feature,\n",
    "    label,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e2e4c-0292-4461-81e1-18f4d068e98c",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccdd7ca9-77d0-4118-876e-df1d8318a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(label, prediction):\n",
    "    cm = confusion_matrix(label, prediction)\n",
    "    TP = cm[1, 1]  # True Positives\n",
    "    TN = cm[0, 0]  # True Negatives\n",
    "    FP = cm[0, 1]  # False Positives\n",
    "    FN = cm[1, 0]  # False Negatives\n",
    "\n",
    "    # Calculate precision and recall for the positive class\n",
    "    precision_pos = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall_pos = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos) if (precision_pos + recall_pos) != 0 else 0\n",
    "\n",
    "    # Calculate precision and recall for the negative class\n",
    "    precision_neg = TN / (TN + FN) if (TN + FN) != 0 else 0\n",
    "    recall_neg = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    f1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg) if (precision_neg + recall_neg) != 0 else 0\n",
    "\n",
    "    # Display in a table\n",
    "    metrics = pd.DataFrame({\n",
    "        'Metric': ['Precision (Positive)', 'Recall (Positive)', 'F1 Score (Positive)',\n",
    "                   'Precision (Negative)', 'Recall (Negative)', 'F1 Score (Negative)'],\n",
    "        'Value': [precision_pos, recall_pos, f1_pos, precision_neg, recall_neg, f1_neg]\n",
    "    })\n",
    "    print(metrics)\n",
    "    return metrics\n",
    "\n",
    "def eval_result(model, feature_test, label_test):\n",
    "    predictions_test = model.predict(feature_test)\n",
    "    predicted_labels_test = (predictions_test > 0.5).astype(int)\n",
    "    metric = get_metric(label_test, predicted_labels_test)\n",
    "    return metric\n",
    "\n",
    "def create_feature(units, dropout, feature_data):\n",
    "    input_feature = Input(shape=(feature_data.shape[1],))\n",
    "    dense_feature = Dense(units, activation='relu')(input_feature)\n",
    "    dropout_feature = Dropout(dropout)(dense_feature)\n",
    "    return input_feature, dropout_feature\n",
    "    \n",
    "def train_lstm(units, dropout, l2, learn_rate, feature_train, label_train,feature_test, label_test):\n",
    "    # Create feature layer\n",
    "    inputs = []\n",
    "    features = []\n",
    "    for feature_data in feature_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # LSTM branch (Processing Article Embeddings)\n",
    "    lstm_art_input = Reshape((1, units))(features[0])\n",
    "    lstm_art = LSTM(units, dropout=dropout, recurrent_dropout=dropout)(lstm_art_input)\n",
    "\n",
    "    # LSTM branch (Processing Lexicon)\n",
    "    lstm_lexicon_input = Reshape((1, units))(features[1])\n",
    "    lstm_lexicon = LSTM(units, dropout=dropout, recurrent_dropout=dropout)(lstm_lexicon_input)\n",
    "\n",
    "    # Concatenate\n",
    "    concat_layer = Concatenate()([lstm_art, lstm_lexicon] + features[2:])\n",
    "    batch_norm = BatchNormalization()(concat_layer)\n",
    "    dense_layer = Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2))(batch_norm)\n",
    "    dropout_dense = Dropout(dropout)(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_dense)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [data for data in feature_train],\n",
    "        label_train,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate Results\n",
    "    metric = eval_result(model, feature_test, label_test)\n",
    "    return model, metric\n",
    "\n",
    "def train_gru(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test):\n",
    "    # Create feature layer\n",
    "    inputs = []\n",
    "    features = []\n",
    "    for feature_data in feature_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # GRU branch\n",
    "    gru_input = Reshape((1, units))(features[0])\n",
    "    gru_layer = GRU(units, dropout=dropout, recurrent_dropout=dropout)(gru_input)\n",
    "\n",
    "    # Concatenate\n",
    "    concat_layer = Concatenate()([gru_layer] + features[1:])\n",
    "    batch_norm = BatchNormalization()(concat_layer)\n",
    "    dense_layer = Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2))(batch_norm)\n",
    "    dropout_dense = Dropout(dropout)(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_dense)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [data for data in feature_train],\n",
    "        label_train,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate Results\n",
    "    print(\"-\"*60)\n",
    "    metric = eval_result(model, feature_test, label_test)\n",
    "    return model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "566b51f6-e578-433a-bcff-364b215b648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "dropout = 0.5\n",
    "l2 = 0.01\n",
    "learn_rate = 0.0001\n",
    "feature_train = [emb_art_train, lexicon_train, max_emb_sent_train]\n",
    "label_train = label_train\n",
    "feature_test = [emb_art_test, lexicon_test, max_emb_sent_test]\n",
    "label_test = label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f304e-2f15-488a-9af2-9fef4e35539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = train_lstm(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b6b7d-10bc-42ac-ada9-82e3e0c507c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = train_gru(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7438e-7c29-4a90-8ef5-c81b3ddec888",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f9ecd6c-d3ab-46f4-9100-8626d059d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "dropout = 0.5\n",
    "l2 = 0.01\n",
    "learn_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89bebbaa-924c-4204-9181-199dda6abd3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1/5\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 15s 64ms/step - loss: 1.7849 - accuracy: 0.5234 - val_loss: 1.5819 - val_accuracy: 0.5664\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.6365 - accuracy: 0.5718 - val_loss: 1.4941 - val_accuracy: 0.6079\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.5304 - accuracy: 0.5903 - val_loss: 1.4058 - val_accuracy: 0.6452\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.4400 - accuracy: 0.6103 - val_loss: 1.3365 - val_accuracy: 0.6515\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.3702 - accuracy: 0.6325 - val_loss: 1.2795 - val_accuracy: 0.6556\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.3062 - accuracy: 0.6428 - val_loss: 1.2219 - val_accuracy: 0.6680\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 1.2494 - accuracy: 0.6424 - val_loss: 1.1812 - val_accuracy: 0.6805\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 1.1983 - accuracy: 0.6638 - val_loss: 1.1375 - val_accuracy: 0.6784\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 1.1596 - accuracy: 0.6613 - val_loss: 1.1046 - val_accuracy: 0.6701\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.1286 - accuracy: 0.6617 - val_loss: 1.0649 - val_accuracy: 0.6888\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.0826 - accuracy: 0.6606 - val_loss: 1.0345 - val_accuracy: 0.6971\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.0403 - accuracy: 0.6807 - val_loss: 1.0079 - val_accuracy: 0.6846\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.0050 - accuracy: 0.7000 - val_loss: 0.9777 - val_accuracy: 0.6888\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.9758 - accuracy: 0.6894 - val_loss: 0.9535 - val_accuracy: 0.6743\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.9444 - accuracy: 0.7030 - val_loss: 0.9284 - val_accuracy: 0.6929\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 7s 54ms/step - loss: 0.9132 - accuracy: 0.7127 - val_loss: 0.9047 - val_accuracy: 0.6971\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.8828 - accuracy: 0.7215 - val_loss: 0.8894 - val_accuracy: 0.6950\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.8763 - accuracy: 0.7092 - val_loss: 0.8651 - val_accuracy: 0.7012\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.8453 - accuracy: 0.7288 - val_loss: 0.8483 - val_accuracy: 0.7075\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.8192 - accuracy: 0.7348 - val_loss: 0.8303 - val_accuracy: 0.7095\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.7973 - accuracy: 0.7360 - val_loss: 0.8202 - val_accuracy: 0.7075\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.7761 - accuracy: 0.7365 - val_loss: 0.8078 - val_accuracy: 0.7075\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.7549 - accuracy: 0.7521 - val_loss: 0.7888 - val_accuracy: 0.7095\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.7396 - accuracy: 0.7514 - val_loss: 0.7743 - val_accuracy: 0.7261\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.7201 - accuracy: 0.7542 - val_loss: 0.7691 - val_accuracy: 0.7116\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.6982 - accuracy: 0.7653 - val_loss: 0.7537 - val_accuracy: 0.7178\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.6860 - accuracy: 0.7678 - val_loss: 0.7461 - val_accuracy: 0.7199\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.6637 - accuracy: 0.7699 - val_loss: 0.7285 - val_accuracy: 0.7303\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 7s 55ms/step - loss: 0.6441 - accuracy: 0.7780 - val_loss: 0.7218 - val_accuracy: 0.7282\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.6291 - accuracy: 0.7807 - val_loss: 0.7215 - val_accuracy: 0.7158\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.6142 - accuracy: 0.7911 - val_loss: 0.7045 - val_accuracy: 0.7282\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.6019 - accuracy: 0.7946 - val_loss: 0.6987 - val_accuracy: 0.7241\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.5875 - accuracy: 0.7925 - val_loss: 0.6942 - val_accuracy: 0.7178\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.5675 - accuracy: 0.7973 - val_loss: 0.6897 - val_accuracy: 0.7220\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5494 - accuracy: 0.8089 - val_loss: 0.6835 - val_accuracy: 0.7303\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5472 - accuracy: 0.8008 - val_loss: 0.6782 - val_accuracy: 0.7324\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.5311 - accuracy: 0.8107 - val_loss: 0.6767 - val_accuracy: 0.7282\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5262 - accuracy: 0.8116 - val_loss: 0.6710 - val_accuracy: 0.7220\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.5113 - accuracy: 0.8165 - val_loss: 0.6749 - val_accuracy: 0.7261\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 7s 55ms/step - loss: 0.4959 - accuracy: 0.8213 - val_loss: 0.6700 - val_accuracy: 0.7324\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4798 - accuracy: 0.8333 - val_loss: 0.6607 - val_accuracy: 0.7407\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4759 - accuracy: 0.8213 - val_loss: 0.6716 - val_accuracy: 0.7241\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4555 - accuracy: 0.8331 - val_loss: 0.6572 - val_accuracy: 0.7427\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4597 - accuracy: 0.8298 - val_loss: 0.6598 - val_accuracy: 0.7365\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4358 - accuracy: 0.8437 - val_loss: 0.6698 - val_accuracy: 0.7220\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4278 - accuracy: 0.8513 - val_loss: 0.6685 - val_accuracy: 0.7386\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4212 - accuracy: 0.8407 - val_loss: 0.6639 - val_accuracy: 0.7344\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4131 - accuracy: 0.8483 - val_loss: 0.6607 - val_accuracy: 0.7344\n",
      "38/38 [==============================] - 1s 11ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.759113\n",
      "1     Recall (Positive)  0.773829\n",
      "2   F1 Score (Positive)  0.766400\n",
      "3  Precision (Negative)  0.756098\n",
      "4     Recall (Negative)  0.740614\n",
      "5   F1 Score (Negative)  0.748276\n",
      "Running Fold 2/5\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 14s 61ms/step - loss: 1.7316 - accuracy: 0.5246 - val_loss: 1.5608 - val_accuracy: 0.5726\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.5989 - accuracy: 0.5654 - val_loss: 1.4690 - val_accuracy: 0.6473\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.4784 - accuracy: 0.6048 - val_loss: 1.3682 - val_accuracy: 0.6846\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 1.4048 - accuracy: 0.6161 - val_loss: 1.2941 - val_accuracy: 0.6888\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.3320 - accuracy: 0.6387 - val_loss: 1.2316 - val_accuracy: 0.6846\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 1.2615 - accuracy: 0.6477 - val_loss: 1.1799 - val_accuracy: 0.6867\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.2200 - accuracy: 0.6486 - val_loss: 1.1341 - val_accuracy: 0.6888\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.1598 - accuracy: 0.6705 - val_loss: 1.0923 - val_accuracy: 0.6971\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.1084 - accuracy: 0.6793 - val_loss: 1.0589 - val_accuracy: 0.6805\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.0669 - accuracy: 0.6883 - val_loss: 1.0195 - val_accuracy: 0.6909\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.0317 - accuracy: 0.6926 - val_loss: 0.9855 - val_accuracy: 0.7054\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.0008 - accuracy: 0.6910 - val_loss: 0.9637 - val_accuracy: 0.6701\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.9551 - accuracy: 0.7060 - val_loss: 0.9313 - val_accuracy: 0.6888\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.9298 - accuracy: 0.7021 - val_loss: 0.9035 - val_accuracy: 0.6867\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.8953 - accuracy: 0.7173 - val_loss: 0.8811 - val_accuracy: 0.6846\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.8854 - accuracy: 0.7120 - val_loss: 0.8613 - val_accuracy: 0.7033\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.8532 - accuracy: 0.7205 - val_loss: 0.8407 - val_accuracy: 0.6950\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.8114 - accuracy: 0.7344 - val_loss: 0.8139 - val_accuracy: 0.6992\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.7879 - accuracy: 0.7376 - val_loss: 0.8034 - val_accuracy: 0.7033\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.7678 - accuracy: 0.7473 - val_loss: 0.7833 - val_accuracy: 0.6971\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.7532 - accuracy: 0.7496 - val_loss: 0.7672 - val_accuracy: 0.7116\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.7328 - accuracy: 0.7418 - val_loss: 0.7483 - val_accuracy: 0.7075\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.7090 - accuracy: 0.7611 - val_loss: 0.7412 - val_accuracy: 0.6971\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.6915 - accuracy: 0.7664 - val_loss: 0.7226 - val_accuracy: 0.6929\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6743 - accuracy: 0.7600 - val_loss: 0.7119 - val_accuracy: 0.7012\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6528 - accuracy: 0.7710 - val_loss: 0.7045 - val_accuracy: 0.7220\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.6404 - accuracy: 0.7761 - val_loss: 0.6935 - val_accuracy: 0.6950\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6282 - accuracy: 0.7768 - val_loss: 0.6808 - val_accuracy: 0.7116\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6053 - accuracy: 0.7853 - val_loss: 0.6780 - val_accuracy: 0.7075\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5929 - accuracy: 0.7846 - val_loss: 0.6663 - val_accuracy: 0.7158\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5781 - accuracy: 0.7918 - val_loss: 0.6629 - val_accuracy: 0.7095\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5634 - accuracy: 0.7955 - val_loss: 0.6568 - val_accuracy: 0.7116\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5525 - accuracy: 0.8010 - val_loss: 0.6551 - val_accuracy: 0.7075\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5473 - accuracy: 0.8015 - val_loss: 0.6461 - val_accuracy: 0.7116\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5218 - accuracy: 0.8169 - val_loss: 0.6323 - val_accuracy: 0.7199\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5187 - accuracy: 0.8128 - val_loss: 0.6300 - val_accuracy: 0.7199\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5077 - accuracy: 0.8123 - val_loss: 0.6252 - val_accuracy: 0.7303\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4911 - accuracy: 0.8218 - val_loss: 0.6245 - val_accuracy: 0.7261\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4811 - accuracy: 0.8259 - val_loss: 0.6139 - val_accuracy: 0.7344\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 7s 54ms/step - loss: 0.4713 - accuracy: 0.8310 - val_loss: 0.6192 - val_accuracy: 0.7178\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4481 - accuracy: 0.8418 - val_loss: 0.6100 - val_accuracy: 0.7303\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.4530 - accuracy: 0.8301 - val_loss: 0.6087 - val_accuracy: 0.7407\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 7s 54ms/step - loss: 0.4331 - accuracy: 0.8485 - val_loss: 0.6121 - val_accuracy: 0.7261\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.4354 - accuracy: 0.8398 - val_loss: 0.6141 - val_accuracy: 0.7220\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4195 - accuracy: 0.8460 - val_loss: 0.6001 - val_accuracy: 0.7365\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.3995 - accuracy: 0.8577 - val_loss: 0.6059 - val_accuracy: 0.7365\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.3987 - accuracy: 0.8545 - val_loss: 0.6063 - val_accuracy: 0.7344\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4012 - accuracy: 0.8483 - val_loss: 0.6064 - val_accuracy: 0.7241\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.3835 - accuracy: 0.8593 - val_loss: 0.6047 - val_accuracy: 0.7241\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.3816 - accuracy: 0.8582 - val_loss: 0.6066 - val_accuracy: 0.7261\n",
      "38/38 [==============================] - 1s 10ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.726154\n",
      "1     Recall (Positive)  0.780165\n",
      "2   F1 Score (Positive)  0.752191\n",
      "3  Precision (Negative)  0.760360\n",
      "4     Recall (Negative)  0.703333\n",
      "5   F1 Score (Negative)  0.730736\n",
      "Running Fold 3/5\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 14s 64ms/step - loss: 1.7998 - accuracy: 0.5312 - val_loss: 1.5628 - val_accuracy: 0.6162\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.6058 - accuracy: 0.5949 - val_loss: 1.4703 - val_accuracy: 0.6328\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 1.5073 - accuracy: 0.6018 - val_loss: 1.3778 - val_accuracy: 0.6805\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.4111 - accuracy: 0.6309 - val_loss: 1.2956 - val_accuracy: 0.6867\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.3442 - accuracy: 0.6412 - val_loss: 1.2301 - val_accuracy: 0.7012\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.2675 - accuracy: 0.6507 - val_loss: 1.1750 - val_accuracy: 0.7075\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 1.2308 - accuracy: 0.6558 - val_loss: 1.1293 - val_accuracy: 0.7012\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 7s 55ms/step - loss: 1.1783 - accuracy: 0.6597 - val_loss: 1.0865 - val_accuracy: 0.7137\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 7s 55ms/step - loss: 1.1271 - accuracy: 0.6754 - val_loss: 1.0513 - val_accuracy: 0.7095\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.0818 - accuracy: 0.6809 - val_loss: 1.0146 - val_accuracy: 0.7137\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.0463 - accuracy: 0.6929 - val_loss: 0.9812 - val_accuracy: 0.7158\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 1.0103 - accuracy: 0.6922 - val_loss: 0.9521 - val_accuracy: 0.7033\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.9808 - accuracy: 0.6922 - val_loss: 0.9222 - val_accuracy: 0.7220\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.9486 - accuracy: 0.7000 - val_loss: 0.8915 - val_accuracy: 0.7241\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.9176 - accuracy: 0.7049 - val_loss: 0.8699 - val_accuracy: 0.7407\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.8899 - accuracy: 0.7178 - val_loss: 0.8502 - val_accuracy: 0.7344\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.8585 - accuracy: 0.7217 - val_loss: 0.8284 - val_accuracy: 0.7303\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.8421 - accuracy: 0.7222 - val_loss: 0.8089 - val_accuracy: 0.7427\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.7986 - accuracy: 0.7325 - val_loss: 0.7935 - val_accuracy: 0.7427\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.7915 - accuracy: 0.7381 - val_loss: 0.7698 - val_accuracy: 0.7427\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.7679 - accuracy: 0.7441 - val_loss: 0.7568 - val_accuracy: 0.7365\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.7454 - accuracy: 0.7514 - val_loss: 0.7370 - val_accuracy: 0.7510\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.7304 - accuracy: 0.7537 - val_loss: 0.7240 - val_accuracy: 0.7552\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.7053 - accuracy: 0.7558 - val_loss: 0.7102 - val_accuracy: 0.7552\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.6763 - accuracy: 0.7690 - val_loss: 0.6975 - val_accuracy: 0.7531\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6741 - accuracy: 0.7674 - val_loss: 0.6881 - val_accuracy: 0.7552\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6439 - accuracy: 0.7743 - val_loss: 0.6794 - val_accuracy: 0.7386\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.6279 - accuracy: 0.7856 - val_loss: 0.6606 - val_accuracy: 0.7531\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6249 - accuracy: 0.7773 - val_loss: 0.6611 - val_accuracy: 0.7552\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.6023 - accuracy: 0.7856 - val_loss: 0.6466 - val_accuracy: 0.7531\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5919 - accuracy: 0.7904 - val_loss: 0.6435 - val_accuracy: 0.7490\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5794 - accuracy: 0.7962 - val_loss: 0.6296 - val_accuracy: 0.7573\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.5567 - accuracy: 0.8026 - val_loss: 0.6274 - val_accuracy: 0.7593\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5441 - accuracy: 0.8063 - val_loss: 0.6203 - val_accuracy: 0.7469\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.5405 - accuracy: 0.7969 - val_loss: 0.6245 - val_accuracy: 0.7469\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5249 - accuracy: 0.8077 - val_loss: 0.6085 - val_accuracy: 0.7407\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5092 - accuracy: 0.8144 - val_loss: 0.6125 - val_accuracy: 0.7531\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5040 - accuracy: 0.8195 - val_loss: 0.6021 - val_accuracy: 0.7469\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.4804 - accuracy: 0.8271 - val_loss: 0.6003 - val_accuracy: 0.7490\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 7s 54ms/step - loss: 0.4729 - accuracy: 0.8308 - val_loss: 0.5962 - val_accuracy: 0.7593\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.4624 - accuracy: 0.8347 - val_loss: 0.5916 - val_accuracy: 0.7448\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4555 - accuracy: 0.8294 - val_loss: 0.5967 - val_accuracy: 0.7386\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4458 - accuracy: 0.8317 - val_loss: 0.5896 - val_accuracy: 0.7490\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4359 - accuracy: 0.8409 - val_loss: 0.6006 - val_accuracy: 0.7407\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4308 - accuracy: 0.8372 - val_loss: 0.5910 - val_accuracy: 0.7427\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.4073 - accuracy: 0.8506 - val_loss: 0.5975 - val_accuracy: 0.7510\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4056 - accuracy: 0.8490 - val_loss: 0.6007 - val_accuracy: 0.7490\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.3865 - accuracy: 0.8584 - val_loss: 0.6042 - val_accuracy: 0.7490\n",
      "38/38 [==============================] - 1s 11ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.728571\n",
      "1     Recall (Positive)  0.774030\n",
      "2   F1 Score (Positive)  0.750613\n",
      "3  Precision (Negative)  0.766957\n",
      "4     Recall (Negative)  0.720588\n",
      "5   F1 Score (Negative)  0.743050\n",
      "Running Fold 4/5\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 14s 62ms/step - loss: 1.8279 - accuracy: 0.4999 - val_loss: 1.5804 - val_accuracy: 0.5747\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 1.6550 - accuracy: 0.5587 - val_loss: 1.4902 - val_accuracy: 0.6369\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.5497 - accuracy: 0.5903 - val_loss: 1.4087 - val_accuracy: 0.6473\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.4430 - accuracy: 0.6265 - val_loss: 1.3323 - val_accuracy: 0.6743\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.3892 - accuracy: 0.6182 - val_loss: 1.2780 - val_accuracy: 0.6888\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.3230 - accuracy: 0.6357 - val_loss: 1.2384 - val_accuracy: 0.6867\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.2786 - accuracy: 0.6343 - val_loss: 1.1791 - val_accuracy: 0.7012\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.2306 - accuracy: 0.6417 - val_loss: 1.1457 - val_accuracy: 0.6950\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 1.1790 - accuracy: 0.6534 - val_loss: 1.1071 - val_accuracy: 0.6971\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 1.1294 - accuracy: 0.6714 - val_loss: 1.0720 - val_accuracy: 0.7033\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 1.0907 - accuracy: 0.6733 - val_loss: 1.0407 - val_accuracy: 0.7054\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 1.0449 - accuracy: 0.6767 - val_loss: 1.0080 - val_accuracy: 0.7095\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 1.0231 - accuracy: 0.6880 - val_loss: 0.9841 - val_accuracy: 0.7012\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.9828 - accuracy: 0.7003 - val_loss: 0.9620 - val_accuracy: 0.7178\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.9553 - accuracy: 0.6993 - val_loss: 0.9413 - val_accuracy: 0.7033\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.9240 - accuracy: 0.7104 - val_loss: 0.9109 - val_accuracy: 0.7241\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.8942 - accuracy: 0.7145 - val_loss: 0.8910 - val_accuracy: 0.7220\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.8758 - accuracy: 0.7194 - val_loss: 0.8638 - val_accuracy: 0.7220\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.8450 - accuracy: 0.7263 - val_loss: 0.8421 - val_accuracy: 0.7407\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.8352 - accuracy: 0.7162 - val_loss: 0.8303 - val_accuracy: 0.7199\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.8025 - accuracy: 0.7422 - val_loss: 0.8245 - val_accuracy: 0.7261\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 7s 55ms/step - loss: 0.7827 - accuracy: 0.7420 - val_loss: 0.7989 - val_accuracy: 0.7407\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.7676 - accuracy: 0.7445 - val_loss: 0.7829 - val_accuracy: 0.7427\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.7457 - accuracy: 0.7452 - val_loss: 0.7720 - val_accuracy: 0.7324\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 7s 55ms/step - loss: 0.7211 - accuracy: 0.7614 - val_loss: 0.7606 - val_accuracy: 0.7261\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.7025 - accuracy: 0.7586 - val_loss: 0.7479 - val_accuracy: 0.7324\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.6969 - accuracy: 0.7611 - val_loss: 0.7335 - val_accuracy: 0.7386\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.6779 - accuracy: 0.7653 - val_loss: 0.7182 - val_accuracy: 0.7365\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.6525 - accuracy: 0.7842 - val_loss: 0.7130 - val_accuracy: 0.7303\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 8s 55ms/step - loss: 0.6366 - accuracy: 0.7768 - val_loss: 0.7011 - val_accuracy: 0.7407\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.6293 - accuracy: 0.7750 - val_loss: 0.6946 - val_accuracy: 0.7469\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.6095 - accuracy: 0.7955 - val_loss: 0.6984 - val_accuracy: 0.7261\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.5892 - accuracy: 0.8006 - val_loss: 0.6816 - val_accuracy: 0.7490\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 8s 56ms/step - loss: 0.5724 - accuracy: 0.7992 - val_loss: 0.6719 - val_accuracy: 0.7407\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5658 - accuracy: 0.7920 - val_loss: 0.6645 - val_accuracy: 0.7365\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5475 - accuracy: 0.8072 - val_loss: 0.6644 - val_accuracy: 0.7407\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.5461 - accuracy: 0.8026 - val_loss: 0.6563 - val_accuracy: 0.7448\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.5367 - accuracy: 0.8065 - val_loss: 0.6597 - val_accuracy: 0.7324\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.5114 - accuracy: 0.8167 - val_loss: 0.6449 - val_accuracy: 0.7531\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.5006 - accuracy: 0.8199 - val_loss: 0.6534 - val_accuracy: 0.7407\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.4915 - accuracy: 0.8229 - val_loss: 0.6518 - val_accuracy: 0.7448\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.4827 - accuracy: 0.8229 - val_loss: 0.6428 - val_accuracy: 0.7407\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4659 - accuracy: 0.8342 - val_loss: 0.6431 - val_accuracy: 0.7427\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4528 - accuracy: 0.8381 - val_loss: 0.6340 - val_accuracy: 0.7573\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.4562 - accuracy: 0.8344 - val_loss: 0.6340 - val_accuracy: 0.7448\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4383 - accuracy: 0.8398 - val_loss: 0.6347 - val_accuracy: 0.7490\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.4296 - accuracy: 0.8421 - val_loss: 0.6429 - val_accuracy: 0.7407\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4141 - accuracy: 0.8506 - val_loss: 0.6332 - val_accuracy: 0.7552\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 9s 63ms/step - loss: 0.4088 - accuracy: 0.8540 - val_loss: 0.6487 - val_accuracy: 0.7490\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4010 - accuracy: 0.8531 - val_loss: 0.6468 - val_accuracy: 0.7365\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.3895 - accuracy: 0.8587 - val_loss: 0.6398 - val_accuracy: 0.7635\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.3710 - accuracy: 0.8688 - val_loss: 0.6538 - val_accuracy: 0.7427\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 9s 63ms/step - loss: 0.3778 - accuracy: 0.8596 - val_loss: 0.6559 - val_accuracy: 0.7365\n",
      "38/38 [==============================] - 1s 11ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.763158\n",
      "1     Recall (Positive)  0.766942\n",
      "2   F1 Score (Positive)  0.765045\n",
      "3  Precision (Negative)  0.763819\n",
      "4     Recall (Negative)  0.760000\n",
      "5   F1 Score (Negative)  0.761905\n",
      "Running Fold 5/5\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 14s 66ms/step - loss: 1.7285 - accuracy: 0.5357 - val_loss: 1.5573 - val_accuracy: 0.5311\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 1.5798 - accuracy: 0.5853 - val_loss: 1.4525 - val_accuracy: 0.6349\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 1.4826 - accuracy: 0.5954 - val_loss: 1.3651 - val_accuracy: 0.6639\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 1.3905 - accuracy: 0.6120 - val_loss: 1.2888 - val_accuracy: 0.6701\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 1.3031 - accuracy: 0.6351 - val_loss: 1.2371 - val_accuracy: 0.6909\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 1.2513 - accuracy: 0.6399 - val_loss: 1.1898 - val_accuracy: 0.6701\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.1881 - accuracy: 0.6568 - val_loss: 1.1435 - val_accuracy: 0.6888\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.1414 - accuracy: 0.6692 - val_loss: 1.0991 - val_accuracy: 0.6846\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.0982 - accuracy: 0.6736 - val_loss: 1.0599 - val_accuracy: 0.6784\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 1.0570 - accuracy: 0.6800 - val_loss: 1.0336 - val_accuracy: 0.6867\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 1.0053 - accuracy: 0.6955 - val_loss: 0.9983 - val_accuracy: 0.6867\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.9817 - accuracy: 0.6897 - val_loss: 0.9702 - val_accuracy: 0.6846\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.9498 - accuracy: 0.6996 - val_loss: 0.9444 - val_accuracy: 0.6909\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 9s 63ms/step - loss: 0.9165 - accuracy: 0.7075 - val_loss: 0.9272 - val_accuracy: 0.6888\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.8965 - accuracy: 0.7063 - val_loss: 0.9048 - val_accuracy: 0.6909\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.8716 - accuracy: 0.7137 - val_loss: 0.8775 - val_accuracy: 0.6909\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.8377 - accuracy: 0.7301 - val_loss: 0.8593 - val_accuracy: 0.6929\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.8207 - accuracy: 0.7319 - val_loss: 0.8403 - val_accuracy: 0.7012\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.7919 - accuracy: 0.7379 - val_loss: 0.8240 - val_accuracy: 0.6950\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 9s 63ms/step - loss: 0.7745 - accuracy: 0.7397 - val_loss: 0.8092 - val_accuracy: 0.6971\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.7556 - accuracy: 0.7455 - val_loss: 0.7936 - val_accuracy: 0.6950\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.7320 - accuracy: 0.7522 - val_loss: 0.7856 - val_accuracy: 0.6743\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.7164 - accuracy: 0.7522 - val_loss: 0.7721 - val_accuracy: 0.6909\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.7007 - accuracy: 0.7556 - val_loss: 0.7615 - val_accuracy: 0.6888\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.6805 - accuracy: 0.7593 - val_loss: 0.7677 - val_accuracy: 0.6950\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 9s 63ms/step - loss: 0.6678 - accuracy: 0.7614 - val_loss: 0.7336 - val_accuracy: 0.7012\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.6476 - accuracy: 0.7674 - val_loss: 0.7373 - val_accuracy: 0.6971\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.6348 - accuracy: 0.7716 - val_loss: 0.7237 - val_accuracy: 0.7012\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.6191 - accuracy: 0.7741 - val_loss: 0.7229 - val_accuracy: 0.6950\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.5998 - accuracy: 0.7877 - val_loss: 0.7105 - val_accuracy: 0.6971\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.5928 - accuracy: 0.7870 - val_loss: 0.7103 - val_accuracy: 0.6992\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.5818 - accuracy: 0.7891 - val_loss: 0.7009 - val_accuracy: 0.6867\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.5645 - accuracy: 0.7870 - val_loss: 0.6905 - val_accuracy: 0.7075\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.5527 - accuracy: 0.7962 - val_loss: 0.6867 - val_accuracy: 0.7012\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.5379 - accuracy: 0.8071 - val_loss: 0.6950 - val_accuracy: 0.6971\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.5195 - accuracy: 0.8061 - val_loss: 0.6905 - val_accuracy: 0.7137\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.5113 - accuracy: 0.8103 - val_loss: 0.6790 - val_accuracy: 0.7158\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4921 - accuracy: 0.8220 - val_loss: 0.6881 - val_accuracy: 0.6992\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4885 - accuracy: 0.8227 - val_loss: 0.6781 - val_accuracy: 0.7075\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4957 - accuracy: 0.8128 - val_loss: 0.6783 - val_accuracy: 0.7012\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4751 - accuracy: 0.8257 - val_loss: 0.6710 - val_accuracy: 0.7075\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 8s 57ms/step - loss: 0.4562 - accuracy: 0.8308 - val_loss: 0.6741 - val_accuracy: 0.7012\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4561 - accuracy: 0.8310 - val_loss: 0.6644 - val_accuracy: 0.7075\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4428 - accuracy: 0.8271 - val_loss: 0.6941 - val_accuracy: 0.6992\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.4437 - accuracy: 0.8273 - val_loss: 0.6783 - val_accuracy: 0.6992\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4142 - accuracy: 0.8428 - val_loss: 0.6739 - val_accuracy: 0.6950\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.4148 - accuracy: 0.8476 - val_loss: 0.6794 - val_accuracy: 0.7012\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.4051 - accuracy: 0.8476 - val_loss: 0.6833 - val_accuracy: 0.6971\n",
      "38/38 [==============================] - 1s 11ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.731895\n",
      "1     Recall (Positive)  0.805085\n",
      "2   F1 Score (Positive)  0.766747\n",
      "3  Precision (Negative)  0.792793\n",
      "4     Recall (Negative)  0.716612\n",
      "5   F1 Score (Negative)  0.752780\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "collect_metric = []\n",
    "X = range(len(merged_undersample))\n",
    "\n",
    "start_time = time.time()\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Running Fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Splitting each feature into training and validation sets\n",
    "    lexicon_train, lexicon_val = lexicon_feature.iloc[train_idx], lexicon_feature.iloc[val_idx]\n",
    "    article_emb_train, article_emb_val = article_emb_feature[train_idx], article_emb_feature[val_idx]\n",
    "    max_sentence_emb_train, max_sentence_emb_val = max_sentence_emb_feature[train_idx], max_sentence_emb_feature[val_idx]\n",
    "    min_sentence_emb_train, min_sentence_emb_val = min_sentence_emb_feature[train_idx], min_sentence_emb_feature[val_idx]\n",
    "    emotion_train, emotion_val = emotion_feature[train_idx], emotion_feature[val_idx]\n",
    "    cosine_train, cosine_val = cosine_feature[train_idx], cosine_feature[val_idx]\n",
    "    label_train, label_val = label[train_idx], label[val_idx]\n",
    "\n",
    "    # Combine individual feature sets for training and validation\n",
    "    feature_train = [article_emb_train, lexicon_train, max_sentence_emb_train]\n",
    "    feature_val = [article_emb_val, lexicon_val, max_sentence_emb_val]\n",
    "\n",
    "    # Train the model\n",
    "    lstm_model, metric = train_lstm(units, dropout, l2, learn_rate, feature_train, label_train, feature_val, label_val)\n",
    "    collect_metric.append(metric)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92eee568-fcc2-4795-a811-c6eaa57c5f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.741778</td>\n",
       "      <td>Precision (Positive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780010</td>\n",
       "      <td>Recall (Positive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.760199</td>\n",
       "      <td>F1 Score (Positive)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.768005</td>\n",
       "      <td>Precision (Negative)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.728230</td>\n",
       "      <td>Recall (Negative)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.747349</td>\n",
       "      <td>F1 Score (Negative)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Value                Metric\n",
       "0  0.741778  Precision (Positive)\n",
       "1  0.780010     Recall (Positive)\n",
       "2  0.760199   F1 Score (Positive)\n",
       "3  0.768005  Precision (Negative)\n",
       "4  0.728230     Recall (Negative)\n",
       "5  0.747349   F1 Score (Negative)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = comb_metric.select_dtypes(include='number')\n",
    "final_metric = numeric_cols.groupby(numeric_cols.index).mean()\n",
    "final_metric['Metric'] = collect_metric[0]['Metric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c686c8-6557-4208-8174-955da5237648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lonely",
   "language": "python",
   "name": "lonely"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
