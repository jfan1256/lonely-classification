{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "21e0bceb-3595-4277-9a58-92e4fa17afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from keras.layers import Input, Embedding, LSTM, GRU, Dense, Dropout, Concatenate, BatchNormalization, Bidirectional, Reshape\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from utils.system import *\n",
    "from metric import get_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1a386f0e-37b2-437e-b7ea-9908e0f0726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14287166054919514467\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14267973632\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7454526811992541762\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68eebe4-279c-451d-8918-9ef517c11ff1",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "93711d11-08b3-4629-b515-154ea43efa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "article = pd.read_csv(get_data() / 'human_annotations_all_8000_overall.csv')\n",
    "art_emb = pd.read_parquet(get_data() / 'bert_article_emb.parquet.brotli')\n",
    "sentence_emb = pd.read_parquet(get_data() / 'bert_sentence_cosine.parquet.brotli')  \n",
    "sent = pd.read_parquet(get_data() / 'bert_sentiment.parquet.brotli')\n",
    "art_cos = pd.read_parquet(get_data() / 'bert_art_cosine.parquet.brotli')  \n",
    "emotion = pd.read_parquet(get_data() / 'bert_emotion.parquet.brotli')\n",
    "topic = pd.read_parquet(get_data() / 'lda_topic.parquet.brotli')\n",
    "n_gram = pd.read_parquet(get_data() / 'n_gram.parquet.brotli')\n",
    "lex_div = pd.read_parquet(get_data() / 'lexical_div.parquet.brotli')\n",
    "readability = pd.read_parquet(get_data() / 'readability.parquet.brotli')\n",
    "time = pd.read_parquet(get_data() / 'time.parquet.brotli')\n",
    "lexicon = pd.read_parquet(get_data() / 'bert_word_cosine.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f23b1ea6-a4d1-4823-8803-fdff743508f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data together\n",
    "merged_emb = (pd.merge(art_emb, sentence_emb, on='id', how='inner')\n",
    "              .merge(sent, on='id', how='inner')\n",
    "              .merge(art_cos, on='id', how='inner')\n",
    "              .merge(emotion, on='id', how='inner')\n",
    "              .merge(lex_div, on='id', how='inner')\n",
    "              .merge(topic, on='id', how='inner')\n",
    "              .merge(n_gram, on='id', how='inner')\n",
    "              .merge(time, on='id', how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d62892fd-4458-4367-9d6d-345b42a800b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top 500 words\n",
    "lexicon = lexicon.head(500)\n",
    "lexicon = lexicon.reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5ce52-8dec-4e77-b61c-a6a851f6161f",
   "metadata": {},
   "source": [
    "### Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "fce28b7d-e991-439f-93af-e4f5e43a0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = merged_emb.sort_values('overall_label')\n",
    "df_class_0 = undersample[undersample['overall_label'] == 0]\n",
    "df_class_1 = undersample[undersample['overall_label'] == 1]\n",
    "n_samples = min(len(df_class_0), len(df_class_1))\n",
    "# Randomly sample from each class\n",
    "df_class_0_under = df_class_0.sample(n_samples)\n",
    "df_class_1_under = df_class_1.sample(n_samples)\n",
    "# Combine the two dataframes\n",
    "merged_undersample = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "# Shuffle the balanced dataset\n",
    "merged_undersample = merged_undersample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd6b25-d823-454b-96ab-2a350cd78905",
   "metadata": {},
   "source": [
    "### Convert Lexicon Dictionary to Numerical Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "0cb03b53-ec72-4d13-b3ad-0c8c79e31072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_features(article, lexicon):\n",
    "    features = {}\n",
    "    words = set(article.split())\n",
    "    for word in lexicon:\n",
    "        features[f'binary_{word}'] = word in words\n",
    "    return features\n",
    "\n",
    "def create_count_features(article, lexicon):\n",
    "    features = {}\n",
    "    word_counts = Counter(article.split())\n",
    "    for word in lexicon:\n",
    "        features[f'count_{word}'] = word_counts[word]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c6e01f64-de82-423c-b1c3-50f672a0d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_list = lexicon['word'].tolist()\n",
    "# For binary features\n",
    "merged_undersample['binary_features'] = merged_undersample['cleaned_article'].apply(lambda x: create_binary_features(x, lexicon_list))\n",
    "# For count features\n",
    "merged_undersample['count_features'] = merged_undersample['cleaned_article'].apply(lambda x: create_count_features(x, lexicon_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae93a6-9096-4def-bd8a-0c33405fb1e9",
   "metadata": {},
   "source": [
    "#### Format Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e410c0e1-fc4d-4a0a-b675-befbd51a6cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'overall_label', 'cleaned_article', 'bert_emb_art',\n",
       "       'bert_emb_min', 'bert_emb_max', 'sent_score', 'cosine_sim_art_mean',\n",
       "       'cosine_sim_0', 'cosine_sim_1', 'cosine_sim_2', 'cosine_sim_3',\n",
       "       'cosine_sim_4', 'emotion_num', 'ttr', 'Topic_0', 'Topic_1', 'Topic_2',\n",
       "       'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8',\n",
       "       'Topic_9', 'n_gram_1', 'n_gram_2', 'n_gram_3', 'n_gram_4', 'n_gram_5',\n",
       "       'n_gram_6', 'n_gram_7', 'n_gram_8', 'n_gram_9', 'n_gram_10',\n",
       "       'n_gram_11', 'n_gram_12', 'n_gram_13', 'n_gram_14', 'n_gram_15',\n",
       "       'n_gram_16', 'n_gram_17', 'n_gram_18', 'n_gram_19', 'n_gram_20',\n",
       "       'n_gram_21', 'n_gram_22', 'n_gram_23', 'n_gram_24', 'n_gram_25',\n",
       "       'n_gram_26', 'n_gram_27', 'n_gram_28', 'n_gram_29', 'n_gram_30',\n",
       "       'n_gram_31', 'n_gram_32', 'n_gram_33', 'n_gram_34', 'n_gram_35',\n",
       "       'n_gram_36', 'n_gram_37', 'n_gram_38', 'n_gram_39', 'n_gram_40',\n",
       "       'time_reference_count', 'binary_features', 'count_features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_undersample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4769917f-57cb-4895-99a0-3b7ddcac836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_feature = pd.json_normalize(merged_undersample['count_features'])\n",
    "article_emb_feature = np.stack(merged_undersample['bert_emb_art'].to_numpy())\n",
    "max_sentence_emb_feature = np.stack(merged_undersample['bert_emb_max'].to_numpy())\n",
    "min_sentence_emb_feature = np.stack(merged_undersample['bert_emb_min'].to_numpy())\n",
    "emotion_feature = np.array(merged_undersample['emotion_num']).reshape(-1, 1)\n",
    "cosine_feature = np.array(merged_undersample['cosine_sim_art_mean']).reshape(-1, 1)\n",
    "\n",
    "label = merged_undersample['overall_label'].to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87087d65-9724-425f-a0a9-f80deb2f7d08",
   "metadata": {},
   "source": [
    "#### Out of Sample Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a6662530-0a7b-4a38-8053-21f1d17ccad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "emb_art_train, emb_art_test, max_emb_sent_train, max_emb_sent_test, min_emb_sent_train, min_emb_sent_test, lexicon_train, lexicon_test, emotion_train, emotion_test, cosine_train, cosine_test, label_train, label_test = train_test_split(\n",
    "    article_emb_feature,\n",
    "    max_sentence_emb_feature,\n",
    "    min_sentence_emb_feature,\n",
    "    lexicon_feature,\n",
    "    emotion_feature,\n",
    "    cosine_feature,\n",
    "    label,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e2e4c-0292-4461-81e1-18f4d068e98c",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ccdd7ca9-77d0-4118-876e-df1d8318a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(label, prediction):\n",
    "    cm = confusion_matrix(label, prediction)\n",
    "    TP = cm[1, 1]  # True Positives\n",
    "    TN = cm[0, 0]  # True Negatives\n",
    "    FP = cm[0, 1]  # False Positives\n",
    "    FN = cm[1, 0]  # False Negatives\n",
    "\n",
    "    # Calculate precision and recall for the positive class\n",
    "    precision_pos = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall_pos = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos) if (precision_pos + recall_pos) != 0 else 0\n",
    "\n",
    "    # Calculate precision and recall for the negative class\n",
    "    precision_neg = TN / (TN + FN) if (TN + FN) != 0 else 0\n",
    "    recall_neg = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    f1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg) if (precision_neg + recall_neg) != 0 else 0\n",
    "\n",
    "    # Display in a table\n",
    "    metrics = pd.DataFrame({\n",
    "        'Metric': ['Precision (Positive)', 'Recall (Positive)', 'F1 Score (Positive)',\n",
    "                   'Precision (Negative)', 'Recall (Negative)', 'F1 Score (Negative)'],\n",
    "        'Value': [precision_pos, recall_pos, f1_pos, precision_neg, recall_neg, f1_neg]\n",
    "    })\n",
    "    print(metrics)\n",
    "    return metrics\n",
    "\n",
    "def eval_result(model, feature_test, label_test):\n",
    "    predictions_test = model.predict(feature_test)\n",
    "    predicted_labels_test = (predictions_test > 0.5).astype(int)\n",
    "    metric = get_metric(label_test, predicted_labels_test)\n",
    "    return metric\n",
    "\n",
    "def create_feature(units, dropout, feature_data):\n",
    "    input_feature = Input(shape=(feature_data.shape[1],))\n",
    "    dense_feature = Dense(units, activation='relu')(input_feature)\n",
    "    dropout_feature = Dropout(dropout)(dense_feature)\n",
    "    return input_feature, dropout_feature\n",
    "    \n",
    "def train_lstm(units, dropout, l2, learn_rate, feature_train, label_train,feature_test, label_test):\n",
    "    # Create feature layer\n",
    "    inputs = []\n",
    "    features = []\n",
    "    for feature_data in feature_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # LSTM branch (Processing Article Embeddings)\n",
    "    lstm_art_input = Reshape((1, units))(features[0])\n",
    "    lstm_art = LSTM(units, dropout=dropout, recurrent_dropout=dropout)(lstm_art_input)\n",
    "\n",
    "    # LSTM branch (Processing Lexicon)\n",
    "    lstm_lexicon_input = Reshape((1, units))(features[1])\n",
    "    lstm_lexicon = LSTM(units, dropout=dropout, recurrent_dropout=dropout)(lstm_lexicon_input)\n",
    "\n",
    "    # Concatenate\n",
    "    concat_layer = Concatenate()([lstm_art, lstm_lexicon] + features[2:])\n",
    "    batch_norm = BatchNormalization()(concat_layer)\n",
    "    dense_layer = Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2))(batch_norm)\n",
    "    dropout_dense = Dropout(dropout)(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_dense)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [data for data in feature_train],\n",
    "        label_train,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate Results\n",
    "    metric = eval_result(model, feature_test, label_test)\n",
    "    return model, metric\n",
    "\n",
    "def train_gru(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test):\n",
    "    # Create feature layer\n",
    "    inputs = []\n",
    "    features = []\n",
    "    for feature_data in feature_train:\n",
    "        input, feature = create_feature(units, dropout, feature_data)\n",
    "        inputs.append(input)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # GRU branch\n",
    "    gru_input = Reshape((1, units))(features[0])\n",
    "    gru_layer = GRU(units, dropout=dropout, recurrent_dropout=dropout)(gru_input)\n",
    "\n",
    "    # Concatenate\n",
    "    concat_layer = Concatenate()([gru_layer] + features[1:])\n",
    "    batch_norm = BatchNormalization()(concat_layer)\n",
    "    dense_layer = Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2))(batch_norm)\n",
    "    dropout_dense = Dropout(dropout)(dense_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_dense)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        [data for data in feature_train],\n",
    "        label_train,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate Results\n",
    "    print(\"-\"*60)\n",
    "    metric = eval_result(model, feature_test, label_test)\n",
    "    return model, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "566b51f6-e578-433a-bcff-364b215b648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "dropout = 0.5\n",
    "l2 = 0.01\n",
    "learn_rate = 0.0001\n",
    "feature_train = [emb_art_train, ma`x_emb_sent_train, lexicon_train]\n",
    "label_train = label_train\n",
    "feature_test = [emb_art_test, max_emb_sent_test, lexicon_test]\n",
    "label_test = label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "521dcbc0-7b35-424a-b65f-7bea765b0fd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1000\n",
      "136/136 [==============================] - 5s 13ms/step - loss: 1.8436 - accuracy: 0.5301 - val_loss: 1.5785 - val_accuracy: 0.5954\n",
      "Epoch 2/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 1.6465 - accuracy: 0.5868 - val_loss: 1.4790 - val_accuracy: 0.6743\n",
      "Epoch 3/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 1.5399 - accuracy: 0.6142 - val_loss: 1.3934 - val_accuracy: 0.6992\n",
      "Epoch 4/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 1.4624 - accuracy: 0.6375 - val_loss: 1.3326 - val_accuracy: 0.6929\n",
      "Epoch 5/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 1.4080 - accuracy: 0.6588 - val_loss: 1.2776 - val_accuracy: 0.6950\n",
      "Epoch 6/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 1.3373 - accuracy: 0.6555 - val_loss: 1.2345 - val_accuracy: 0.7033\n",
      "Epoch 7/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 1.2807 - accuracy: 0.6705 - val_loss: 1.1914 - val_accuracy: 0.7137\n",
      "Epoch 8/1000\n",
      "136/136 [==============================] - 2s 11ms/step - loss: 1.2405 - accuracy: 0.6719 - val_loss: 1.1515 - val_accuracy: 0.7012\n",
      "Epoch 9/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 1.1918 - accuracy: 0.6751 - val_loss: 1.1157 - val_accuracy: 0.6909\n",
      "Epoch 10/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 1.1455 - accuracy: 0.6809 - val_loss: 1.0830 - val_accuracy: 0.7137\n",
      "Epoch 11/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 1.1130 - accuracy: 0.6862 - val_loss: 1.0527 - val_accuracy: 0.7116\n",
      "Epoch 12/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 1.0776 - accuracy: 0.6876 - val_loss: 1.0213 - val_accuracy: 0.7220\n",
      "Epoch 13/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 1.0276 - accuracy: 0.7097 - val_loss: 0.9921 - val_accuracy: 0.7220\n",
      "Epoch 14/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.9984 - accuracy: 0.7056 - val_loss: 0.9622 - val_accuracy: 0.7344\n",
      "Epoch 15/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9768 - accuracy: 0.7095 - val_loss: 0.9380 - val_accuracy: 0.7386\n",
      "Epoch 16/1000\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.9502 - accuracy: 0.7118 - val_loss: 0.9135 - val_accuracy: 0.7324\n",
      "Epoch 17/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.9228 - accuracy: 0.7127 - val_loss: 0.8913 - val_accuracy: 0.7344\n",
      "Epoch 18/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 0.8970 - accuracy: 0.7228 - val_loss: 0.8746 - val_accuracy: 0.7386\n",
      "Epoch 19/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 0.8579 - accuracy: 0.7252 - val_loss: 0.8549 - val_accuracy: 0.7241\n",
      "Epoch 20/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.8469 - accuracy: 0.7249 - val_loss: 0.8395 - val_accuracy: 0.7303\n",
      "Epoch 21/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.8256 - accuracy: 0.7312 - val_loss: 0.8205 - val_accuracy: 0.7386\n",
      "Epoch 22/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.8017 - accuracy: 0.7401 - val_loss: 0.8039 - val_accuracy: 0.7448\n",
      "Epoch 23/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.7827 - accuracy: 0.7424 - val_loss: 0.7909 - val_accuracy: 0.7282\n",
      "Epoch 24/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.7725 - accuracy: 0.7397 - val_loss: 0.7778 - val_accuracy: 0.7365\n",
      "Epoch 25/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.7526 - accuracy: 0.7475 - val_loss: 0.7615 - val_accuracy: 0.7324\n",
      "Epoch 26/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.7405 - accuracy: 0.7397 - val_loss: 0.7543 - val_accuracy: 0.7365\n",
      "Epoch 27/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.7195 - accuracy: 0.7489 - val_loss: 0.7442 - val_accuracy: 0.7241\n",
      "Epoch 28/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.6976 - accuracy: 0.7607 - val_loss: 0.7283 - val_accuracy: 0.7386\n",
      "Epoch 29/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 0.6943 - accuracy: 0.7517 - val_loss: 0.7164 - val_accuracy: 0.7344\n",
      "Epoch 30/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.6825 - accuracy: 0.7595 - val_loss: 0.7063 - val_accuracy: 0.7407\n",
      "Epoch 31/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 0.6540 - accuracy: 0.7697 - val_loss: 0.6974 - val_accuracy: 0.7427\n",
      "Epoch 32/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.6528 - accuracy: 0.7644 - val_loss: 0.6881 - val_accuracy: 0.7344\n",
      "Epoch 33/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.6448 - accuracy: 0.7644 - val_loss: 0.6812 - val_accuracy: 0.7448\n",
      "Epoch 34/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.6221 - accuracy: 0.7717 - val_loss: 0.6771 - val_accuracy: 0.7220\n",
      "Epoch 35/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.6141 - accuracy: 0.7729 - val_loss: 0.6663 - val_accuracy: 0.7303\n",
      "Epoch 36/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.6010 - accuracy: 0.7793 - val_loss: 0.6594 - val_accuracy: 0.7386\n",
      "Epoch 37/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.5891 - accuracy: 0.7893 - val_loss: 0.6524 - val_accuracy: 0.7344\n",
      "Epoch 38/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.5877 - accuracy: 0.7793 - val_loss: 0.6478 - val_accuracy: 0.7324\n",
      "Epoch 39/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.5747 - accuracy: 0.7810 - val_loss: 0.6421 - val_accuracy: 0.7303\n",
      "Epoch 40/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.5613 - accuracy: 0.7902 - val_loss: 0.6344 - val_accuracy: 0.7386\n",
      "Epoch 41/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.5574 - accuracy: 0.7863 - val_loss: 0.6262 - val_accuracy: 0.7407\n",
      "Epoch 42/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.5454 - accuracy: 0.7957 - val_loss: 0.6191 - val_accuracy: 0.7469\n",
      "Epoch 43/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.5403 - accuracy: 0.7916 - val_loss: 0.6149 - val_accuracy: 0.7573\n",
      "Epoch 44/1000\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.5332 - accuracy: 0.7955 - val_loss: 0.6169 - val_accuracy: 0.7324\n",
      "Epoch 45/1000\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.5207 - accuracy: 0.7999 - val_loss: 0.6107 - val_accuracy: 0.7448\n",
      "Epoch 46/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.5093 - accuracy: 0.8040 - val_loss: 0.6092 - val_accuracy: 0.7448\n",
      "Epoch 47/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 0.5107 - accuracy: 0.8026 - val_loss: 0.6027 - val_accuracy: 0.7365\n",
      "Epoch 48/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.4980 - accuracy: 0.8065 - val_loss: 0.6051 - val_accuracy: 0.7448\n",
      "Epoch 49/1000\n",
      "136/136 [==============================] - 2s 11ms/step - loss: 0.4854 - accuracy: 0.8130 - val_loss: 0.5972 - val_accuracy: 0.7552\n",
      "Epoch 50/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.4864 - accuracy: 0.8148 - val_loss: 0.6025 - val_accuracy: 0.7427\n",
      "Epoch 51/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.4755 - accuracy: 0.8176 - val_loss: 0.5971 - val_accuracy: 0.7469\n",
      "Epoch 52/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.4793 - accuracy: 0.8167 - val_loss: 0.5906 - val_accuracy: 0.7365\n",
      "Epoch 53/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.4665 - accuracy: 0.8188 - val_loss: 0.5984 - val_accuracy: 0.7490\n",
      "Epoch 54/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.4564 - accuracy: 0.8266 - val_loss: 0.5967 - val_accuracy: 0.7344\n",
      "Epoch 55/1000\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.4554 - accuracy: 0.8236 - val_loss: 0.5854 - val_accuracy: 0.7469\n",
      "Epoch 56/1000\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 0.4426 - accuracy: 0.8280 - val_loss: 0.5885 - val_accuracy: 0.7427\n",
      "Epoch 57/1000\n",
      "136/136 [==============================] - 2s 11ms/step - loss: 0.4429 - accuracy: 0.8275 - val_loss: 0.5859 - val_accuracy: 0.7510\n",
      "Epoch 58/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 0.4217 - accuracy: 0.8395 - val_loss: 0.5801 - val_accuracy: 0.7552\n",
      "Epoch 59/1000\n",
      "136/136 [==============================] - 1s 11ms/step - loss: 0.4123 - accuracy: 0.8402 - val_loss: 0.5816 - val_accuracy: 0.7490\n",
      "Epoch 60/1000\n",
      "136/136 [==============================] - 2s 11ms/step - loss: 0.4360 - accuracy: 0.8229 - val_loss: 0.5860 - val_accuracy: 0.7427\n",
      "Epoch 61/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.4193 - accuracy: 0.8400 - val_loss: 0.5821 - val_accuracy: 0.7427\n",
      "Epoch 62/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.4102 - accuracy: 0.8444 - val_loss: 0.5790 - val_accuracy: 0.7448\n",
      "Epoch 63/1000\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.4145 - accuracy: 0.8425 - val_loss: 0.5855 - val_accuracy: 0.7427\n",
      "Epoch 64/1000\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.3910 - accuracy: 0.8529 - val_loss: 0.5861 - val_accuracy: 0.7531\n",
      "Epoch 65/1000\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.3909 - accuracy: 0.8508 - val_loss: 0.5912 - val_accuracy: 0.7448\n",
      "Epoch 66/1000\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.4003 - accuracy: 0.8439 - val_loss: 0.5944 - val_accuracy: 0.7282\n",
      "Epoch 67/1000\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.3776 - accuracy: 0.8554 - val_loss: 0.5992 - val_accuracy: 0.7448\n",
      "38/38 [==============================] - 0s 3ms/step\n",
      "                 Metric     Value\n",
      "0  Precision (Positive)  0.697865\n",
      "1     Recall (Positive)  0.735294\n",
      "2   F1 Score (Positive)  0.716091\n",
      "3  Precision (Negative)  0.743289\n",
      "4     Recall (Negative)  0.706539\n",
      "5   F1 Score (Negative)  0.724448\n"
     ]
    }
   ],
   "source": [
    "lstm_model = train_lstm(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a4074-785b-4c21-9ad1-8cb0924f4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = train_gru(units, dropout, l2, learn_rate, feature_train, label_train, feature_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lonely",
   "language": "python",
   "name": "lonely"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
