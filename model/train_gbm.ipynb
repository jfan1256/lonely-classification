{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15c5d0c6-6ce4-4458-98dd-90aac040283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from utils.system import *\n",
    "from metric import get_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b627ae-de9f-4c42-9033-652276f134e9",
   "metadata": {},
   "source": [
    "#### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5719cab-580e-4d36-84e8-c8423304e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "article = pd.read_csv(get_data() / 'human_annotations_all_8000_overall.csv')\n",
    "art_emb = pd.read_parquet(get_data() / 'bert_article_emb.parquet.brotli')\n",
    "sentence_emb = pd.read_parquet(get_data() / 'bert_sentence_cosine.parquet.brotli')  \n",
    "sent = pd.read_parquet(get_data() / 'bert_sentiment.parquet.brotli')\n",
    "art_cos = pd.read_parquet(get_data() / 'bert_art_cosine.parquet.brotli')  \n",
    "emotion = pd.read_parquet(get_data() / 'bert_emotion.parquet.brotli')\n",
    "topic = pd.read_parquet(get_data() / 'lda_topic.parquet.brotli')\n",
    "n_gram = pd.read_parquet(get_data() / 'n_gram.parquet.brotli')\n",
    "lex_div = pd.read_parquet(get_data() / 'lexical_div.parquet.brotli')\n",
    "readability = pd.read_parquet(get_data() / 'readability.parquet.brotli')\n",
    "time_data = pd.read_parquet(get_data() / 'time.parquet.brotli')\n",
    "lexicon = pd.read_parquet(get_data() / 'bert_word_cosine.parquet.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "050c9242-b4ba-4ec7-91b9-5301e9e0f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data together\n",
    "merged_emb = (pd.merge(art_emb, sentence_emb, on='id', how='inner')\n",
    "              .merge(sent, on='id', how='inner')\n",
    "              .merge(art_cos, on='id', how='inner')\n",
    "              .merge(emotion, on='id', how='inner')\n",
    "              .merge(lex_div, on='id', how='inner')\n",
    "              .merge(topic, on='id', how='inner')\n",
    "              .merge(n_gram, on='id', how='inner')\n",
    "              .merge(time_data, on='id', how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1daa2478-f499-4864-9d22-af3e4f425d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack article embeddings + sentence embeddings together into one array\n",
    "merged_emb['comb_emb'] = merged_emb.apply(lambda row: [*row['bert_emb_art'], *row['bert_emb_min'], *row['bert_emb_max']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a356bc63-b17e-4c3b-aef4-37fb10726245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top ____ words\n",
    "lexicon = lexicon.head(800)\n",
    "lexicon = lexicon.reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f4da7-b168-4975-9341-9a36ec905de7",
   "metadata": {},
   "source": [
    "#### Word Count Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a9f83e5-4979-48b3-8231-dc67f0dcc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count\n",
    "merged_emb['word_count'] = merged_emb['cleaned_article'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae85c3c-1f01-4ede-9475-f8ff2339584f",
   "metadata": {},
   "source": [
    "#### Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f2f3234-6216-43b4-bef5-531b3b6b75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample\n",
    "undersample = merged_emb.sort_values('overall_label')\n",
    "df_class_0 = undersample[undersample['overall_label'] == 0]\n",
    "df_class_1 = undersample[undersample['overall_label'] == 1]\n",
    "n_samples = min(len(df_class_0), len(df_class_1))\n",
    "df_class_0_under = df_class_0.sample(n_samples)\n",
    "df_class_1_under = df_class_1.sample(n_samples)\n",
    "merged_undersample = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "merged_undersample = merged_undersample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3917d1-e925-4c53-813f-a2a022e77819",
   "metadata": {},
   "source": [
    "#### Tfidf vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "328b140c-325d-46f3-a998-229bd54d45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(merged_undersample['cleaned_article'])\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df = tfidf_df.drop('count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2402ff44-3f4b-44ef-95c4-d82b97423618",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_undersample = pd.concat([merged_undersample, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8f43c-3a52-4435-9a12-2a6d7292187e",
   "metadata": {},
   "source": [
    "#### Convert Lexicon Dictionary to Numerical Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cb03b53-ec72-4d13-b3ad-0c8c79e31072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_features(article, lexicon):\n",
    "    word_counts = Counter(article.split())\n",
    "    features = [word_counts[word] for word in lexicon]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6e01f64-de82-423c-b1c3-50f672a0d68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon_list = lexicon['word'].tolist()\n",
    "# For count features\n",
    "merged_undersample['count_features'] = merged_undersample['cleaned_article'].apply(lambda x: create_count_features(x, lexicon_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8b349-fffe-461b-9e0b-0feac43c4d5e",
   "metadata": {},
   "source": [
    "#### Flatten word embeddings into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82a2afcc-33f4-4b6a-bad3-5eec8efe44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten an array-containing column\n",
    "def flatten_column(dataframe, column_name):\n",
    "    # Flatten the column\n",
    "    flattened_features = np.stack(dataframe[column_name].values)\n",
    "    # Create a new DataFrame from the flattened features\n",
    "    flattened_df = pd.DataFrame(flattened_features, columns=[f'{column_name}_{i}' for i in range(flattened_features.shape[1])])\n",
    "    return flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd3f42bd-a3e9-40dd-a350-6e0c7420174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each array-containing column\n",
    "flattened_emb_all = flatten_column(merged_undersample, 'bert_emb_art')\n",
    "flattened_emb_max = flatten_column(merged_undersample, 'bert_emb_max')\n",
    "flattened_emb_min = flatten_column(merged_undersample, 'bert_emb_min')\n",
    "flattened_lexicon = flatten_column(merged_undersample, 'count_features')\n",
    "\n",
    "# Concatenate the flattened DataFrames with the rest of the DataFrame\n",
    "merged_undersample = pd.concat([flattened_emb_all, flattened_emb_max, flattened_emb_min, flattened_lexicon,\n",
    "                                merged_undersample.drop(columns=['comb_emb', 'text', 'cleaned_article', 'bert_emb_art', 'bert_emb_max', 'bert_emb_min', 'count_features'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72f8dd-198e-4d24-8255-fd51cb33dc76",
   "metadata": {},
   "source": [
    "#### Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3f96924-b896-446e-b375-8e011c17a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features and label\n",
    "X = merged_undersample.drop('overall_label', axis=1)\n",
    "y = merged_undersample['overall_label']\n",
    "\n",
    "# First split into training (80%) and test (20%)\n",
    "X_train_test, X_test, y_train_test, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split training (80%) into training (70%) and validation (30%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_test, y_train_test, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bf6b815-9340-4ca7-876f-6687a3463fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LightGBM dataset\n",
    "dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=['sent_score', 'emotion_num'], free_raw_data=False)\n",
    "dval = lgb.Dataset(X_val, label=y_val, categorical_feature=['sent_score', 'emotion_num'], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f374b5d-2c8c-464f-8e03-365fef858285",
   "metadata": {},
   "source": [
    "#### Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eedeb62e-f5c1-4595-b778-457f4a64a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "        \"feature_pre_filter\": False,\n",
    "        'device_type': 'gpu', \n",
    "        'gpu_platform_id': 1, \n",
    "        'gpu_device_id': 0, \n",
    "        'gpu_use_dp': True,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbose': -1,\n",
    "        'boosting': 'gbdt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ecabf42-3f17-49a6-bfa1-d11d40d3223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Tunable hyperparameters\n",
    "    param_tuning = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 50),\n",
    "        \"learning_rate\": round(trial.suggest_float(\"learning_rate\", 1e-5, 0.1), 3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "        \"feature_fraction\": round(trial.suggest_float(\"feature_fraction\", 0.1, 1.0), 3),\n",
    "        \"min_gain_to_split\": round(trial.suggest_float(\"min_gain_to_split\", 0, 1.0), 3),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 200),\n",
    "        \"lambda_l1\": round(trial.suggest_float(\"lambda_l1\", 0, 5.0), 3),\n",
    "        \"lambda_l2\": round(trial.suggest_float(\"lambda_l2\", 0, 5.0), 3),\n",
    "    }    \n",
    "\n",
    "    # Combine fixed and tunable hyperparameters\n",
    "    params = {**param_tuning, **fixed_params}\n",
    "\n",
    "    # Train the model\n",
    "    clf = lgb.train(params, dtrain, valid_sets=[dtrain, dval], num_boost_round=1000, callbacks=[lgb.early_stopping(25)])\n",
    "\n",
    "    # Calculate and return the performance metric\n",
    "    y_pred = clf.predict(X_test)\n",
    "    trial.set_user_attr('predictions', y_pred)\n",
    "    # metric = log_loss(y_test, y_pred)\n",
    "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "    f1 = get_metric(y_test, y_pred_binary)\n",
    "    metric = f1.loc[f1['Metric'] == 'F1 Score (Positive)']['Value'].values[0]\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02589cfc-72eb-4a13-bd1a-922b0b58de5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 14:21:50,218] A new study created in memory with name: no-name-ec228034-e7bb-4384-a16b-c31f8e36d904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"-\" * 60)\n",
    "print(\"Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20751084-62d2-46be-a475-210fbc217e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_trial = sorted(study.trials, key=lambda t: t.value, reverse=True)[:1]\n",
    "top_pred = [trial.user_attrs['predictions'] for trial in top_trial]\n",
    "avg_pred = np.mean([pred for pred in top_pred], axis=0)\n",
    "y_pred_binary = (avg_pred >= 0.50).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee427d2-e0bd-44ec-8615-6597e7fadeaa",
   "metadata": {},
   "source": [
    "#### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65841fcd-0261-48b3-9abf-24ecb1f725e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "final_params = {**best_params, **fixed_params}\n",
    "\n",
    "# Train the model\n",
    "lgb_early_stop = lgb.early_stopping(25)\n",
    "clf = lgb.train(final_params, dtrain, valid_sets=[dtrain, dval], num_boost_round=1000, callbacks=[lgb_early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24420b-d0b8-4822-8a7f-2fc4e04a5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred_binary = (y_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7ee27-89ca-4354-bc69-00c8d2fa7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = get_metric(y_test, y_pred_binary)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b19d0-e24f-44a3-b90b-7bda48e32e3c",
   "metadata": {},
   "source": [
    "#### Train Stack Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14080a-39d9-435f-8e63-b16601db6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "y_train_pred_binary = (y_train_pred >= 0.5).astype(int)\n",
    "\n",
    "y_val_pred = clf.predict(X_val)\n",
    "y_val_pred_binary = (y_val_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbb7fd-8c20-4844-b0f4-cc150c3fc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LightGBM dataset\n",
    "dtrain = lgb.Dataset(X_train, label=y_train_pred_binary, categorical_feature=['sent_score', 'emotion_num'], free_raw_data=False)\n",
    "dval = lgb.Dataset(X_val, label=y_val_pred_binary, categorical_feature=['sent_score', 'emotion_num'], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2d013-fdf7-4510-adb3-216469d3ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "final_params = {**best_params, **fixed_params}\n",
    "\n",
    "# Train the model\n",
    "lgb_early_stop = lgb.early_stopping(25)\n",
    "clf = lgb.train(final_params, dtrain, valid_sets=[dtrain, dval], num_boost_round=1000, callbacks=[lgb_early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a40ef-e884-41d8-b7fe-0438ee8563a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred_binary = (y_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d1f90-ea70-4ec4-b322-28704f1e6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = get_metric(y_test, y_pred_binary)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb14437-42f8-4f79-9547-d3a7ecfdb262",
   "metadata": {},
   "source": [
    "#### Plot Gain/Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab87f3-3477-49b0-9ae1-f4fcd50ef8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = clf.feature_importance(importance_type='gain')\n",
    "split = clf.feature_importance(importance_type='split')\n",
    "feature_names = clf.feature_name()\n",
    "importance = pd.DataFrame({\n",
    "    'Feature Name': feature_names,\n",
    "    'Gain': gain,\n",
    "    'Split': split\n",
    "})\n",
    "importance_gain = importance.sort_values(by='Gain', ascending=False)\n",
    "importance_split = importance.sort_values(by='Split', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9f6ce-2dd2-4a5c-91b9-2099e679d0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.barh(importance_gain['Feature Name'].head(100), importance_gain['Gain'].head(100), color='skyblue')\n",
    "plt.xlabel('Gain')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96fa2d-76c3-4bcd-b8ec-d7087dd62b17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.barh(importance_split['Feature Name'].head(100), importance_split['Split'].head(100), color='lightgreen')\n",
    "plt.xlabel('Split')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e2295-611f-4fdc-b68c-7a367e5b14ce",
   "metadata": {},
   "source": [
    "#### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d015df2-4ba1-4e8c-97b2-6d87d57df5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "final_params = {**best_params, **fixed_params}\n",
    "\n",
    "# CV\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "collect_metric = []\n",
    "X = range(len(merged_undersample))\n",
    "\n",
    "# Splitting the data into features and label\n",
    "features = merged_undersample.drop('overall_label', axis=1)\n",
    "label = merged_undersample['overall_label']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Running Fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Splitting each feature into training and validation sets\n",
    "    features_train, features_val = features.iloc[train_idx], features.iloc[val_idx]\n",
    "    label_train, label_val = label[train_idx], label[val_idx]\n",
    "\n",
    "    # Split train into train/validation\n",
    "    features_train_nested, features_val_nested, label_train_nested, label_val_nested = train_test_split(\n",
    "        features_train, label_train, test_size=0.1\n",
    "    )\n",
    "\n",
    "    # Setup LGB Dataset\n",
    "    dtrain_nested = lgb.Dataset(features_train_nested, label=label_train_nested, categorical_feature=['sent_score', 'emotion_num'], free_raw_data=False)\n",
    "    dval_nested = lgb.Dataset(features_val_nested, label=label_val_nested, categorical_feature=['sent_score', 'emotion_num'], free_raw_data=False)    \n",
    "\n",
    "    # Train the model\n",
    "    lgb_early_stop = lgb.early_stopping(25)\n",
    "    clf = lgb.train(final_params, dtrain_nested, valid_sets=[dval_nested], num_boost_round=1000, callbacks=[lgb_early_stop])\n",
    "\n",
    "    # Predict\n",
    "    label_pred = clf.predict(features_val)\n",
    "    label_pred_binary = (label_pred >= 0.5).astype(int)\n",
    "    metric = get_metric(label_val, label_pred_binary)\n",
    "    collect_metric.append(metric)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f392e6-2734-439f-ad97-2c1ac16daa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_metric = pd.concat(collect_metric, axis=0)\n",
    "numeric_cols = comb_metric.select_dtypes(include='number')\n",
    "final_metric = numeric_cols.groupby(numeric_cols.index).mean()\n",
    "final_metric['Metric'] = collect_metric[0]['Metric']\n",
    "final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183df14-3967-499e-8800-c2932e8b3218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lonely",
   "language": "python",
   "name": "lonely"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
